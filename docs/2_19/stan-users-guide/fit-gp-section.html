<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>10.3 Fitting a Gaussian Process | Stan User’s Guide</title>
  <meta name="description" content="Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="10.3 Fitting a Gaussian Process | Stan User’s Guide" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.3 Fitting a Gaussian Process | Stan User’s Guide" />
  
  <meta name="twitter:description" content="Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="simulating-from-a-gaussian-process.html">
<link rel="next" href="directions-rotations-and-hyperspheres.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Stan User's Guide</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 1. Example Models</i></a></li>
<li class="chapter" data-level="1" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>1</b> Regression Models</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>1.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#vectorization.section"><i class="fa fa-check"></i>Matrix Notation and Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="QR-reparameterization-section.html"><a href="QR-reparameterization-section.html"><i class="fa fa-check"></i><b>1.2</b> The QR Reparameterization</a></li>
<li class="chapter" data-level="1.3" data-path="regression-priors-section.html"><a href="regression-priors-section.html"><i class="fa fa-check"></i><b>1.3</b> Priors for Coefficients and Scales</a></li>
<li class="chapter" data-level="1.4" data-path="robust-noise-models.html"><a href="robust-noise-models.html"><i class="fa fa-check"></i><b>1.4</b> Robust Noise Models</a></li>
<li class="chapter" data-level="1.5" data-path="logistic-probit-regression-section.html"><a href="logistic-probit-regression-section.html"><i class="fa fa-check"></i><b>1.5</b> Logistic and Probit Regression</a></li>
<li class="chapter" data-level="1.6" data-path="multi-logit-section.html"><a href="multi-logit-section.html"><i class="fa fa-check"></i><b>1.6</b> Multi-Logit Regression</a><ul>
<li class="chapter" data-level="" data-path="multi-logit-section.html"><a href="multi-logit-section.html#identifiability"><i class="fa fa-check"></i>Identifiability</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="parameterizing-centered-vectors.html"><a href="parameterizing-centered-vectors.html"><i class="fa fa-check"></i><b>1.7</b> Parameterizing Centered Vectors</a><ul>
<li><a href="parameterizing-centered-vectors.html#k-1-degrees-of-freedom"><span class="math inline">\(K-1\)</span> Degrees of Freedom</a></li>
<li class="chapter" data-level="" data-path="parameterizing-centered-vectors.html"><a href="parameterizing-centered-vectors.html#qr-decomposition"><i class="fa fa-check"></i>QR Decomposition</a></li>
<li class="chapter" data-level="" data-path="parameterizing-centered-vectors.html"><a href="parameterizing-centered-vectors.html#translated-and-scaled-simplex"><i class="fa fa-check"></i>Translated and Scaled Simplex</a></li>
<li class="chapter" data-level="" data-path="parameterizing-centered-vectors.html"><a href="parameterizing-centered-vectors.html#soft-centering"><i class="fa fa-check"></i>Soft Centering</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="ordered-logistic-section.html"><a href="ordered-logistic-section.html"><i class="fa fa-check"></i><b>1.8</b> Ordered Logistic and Probit Regression</a><ul>
<li class="chapter" data-level="" data-path="ordered-logistic-section.html"><a href="ordered-logistic-section.html#ordered-logistic-regression"><i class="fa fa-check"></i>Ordered Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="hierarchical-logistic-regression.html"><a href="hierarchical-logistic-regression.html"><i class="fa fa-check"></i><b>1.9</b> Hierarchical Logistic Regression</a></li>
<li class="chapter" data-level="1.10" data-path="hierarchical-priors-section.html"><a href="hierarchical-priors-section.html"><i class="fa fa-check"></i><b>1.10</b> Hierarchical Priors</a><ul>
<li class="chapter" data-level="" data-path="hierarchical-priors-section.html"><a href="hierarchical-priors-section.html#boundary-avoiding-priors-for-mle-in-hierarchical-models"><i class="fa fa-check"></i>Boundary-Avoiding Priors for MLE in Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="item-response-models-section.html"><a href="item-response-models-section.html"><i class="fa fa-check"></i><b>1.11</b> Item-Response Theory Models</a><ul>
<li class="chapter" data-level="" data-path="item-response-models-section.html"><a href="item-response-models-section.html#data-declaration-with-missingness"><i class="fa fa-check"></i>Data Declaration with Missingness</a></li>
<li class="chapter" data-level="" data-path="item-response-models-section.html"><a href="item-response-models-section.html#pl-rasch-model"><i class="fa fa-check"></i>1PL (Rasch) Model</a></li>
<li class="chapter" data-level="" data-path="item-response-models-section.html"><a href="item-response-models-section.html#multilevel-2pl-model"><i class="fa fa-check"></i>Multilevel 2PL Model</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="priors-for-identification-section.html"><a href="priors-for-identification-section.html"><i class="fa fa-check"></i><b>1.12</b> Priors for Identifiability</a><ul>
<li class="chapter" data-level="" data-path="priors-for-identification-section.html"><a href="priors-for-identification-section.html#location-and-scale-invariance"><i class="fa fa-check"></i>Location and Scale Invariance</a></li>
<li class="chapter" data-level="" data-path="priors-for-identification-section.html"><a href="priors-for-identification-section.html#collinearity"><i class="fa fa-check"></i>Collinearity</a></li>
<li class="chapter" data-level="" data-path="priors-for-identification-section.html"><a href="priors-for-identification-section.html#separability"><i class="fa fa-check"></i>Separability</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="multivariate-hierarchical-priors-section.html"><a href="multivariate-hierarchical-priors-section.html"><i class="fa fa-check"></i><b>1.13</b> Multivariate Priors for Hierarchical Models</a><ul>
<li class="chapter" data-level="" data-path="multivariate-hierarchical-priors-section.html"><a href="multivariate-hierarchical-priors-section.html#multivariate-regression-example"><i class="fa fa-check"></i>Multivariate Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="prediction-forecasting-and-backcasting.html"><a href="prediction-forecasting-and-backcasting.html"><i class="fa fa-check"></i><b>1.14</b> Prediction, Forecasting, and Backcasting</a><ul>
<li class="chapter" data-level="" data-path="prediction-forecasting-and-backcasting.html"><a href="prediction-forecasting-and-backcasting.html#programming-predictions"><i class="fa fa-check"></i>Programming Predictions</a></li>
<li class="chapter" data-level="" data-path="prediction-forecasting-and-backcasting.html"><a href="prediction-forecasting-and-backcasting.html#predictions-as-generated-quantities"><i class="fa fa-check"></i>Predictions as Generated Quantities</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="multivariate-outcomes.html"><a href="multivariate-outcomes.html"><i class="fa fa-check"></i><b>1.15</b> Multivariate Outcomes</a><ul>
<li class="chapter" data-level="" data-path="multivariate-outcomes.html"><a href="multivariate-outcomes.html#seemingly-unrelated-regressions"><i class="fa fa-check"></i>Seemingly Unrelated Regressions</a></li>
<li class="chapter" data-level="" data-path="multivariate-outcomes.html"><a href="multivariate-outcomes.html#multivariate-probit-regression"><i class="fa fa-check"></i>Multivariate Probit Regression</a></li>
</ul></li>
<li class="chapter" data-level="1.16" data-path="applications-of-pseudorandom-number-generation.html"><a href="applications-of-pseudorandom-number-generation.html"><i class="fa fa-check"></i><b>1.16</b> Applications of Pseudorandom Number Generation</a><ul>
<li class="chapter" data-level="" data-path="applications-of-pseudorandom-number-generation.html"><a href="applications-of-pseudorandom-number-generation.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="applications-of-pseudorandom-number-generation.html"><a href="applications-of-pseudorandom-number-generation.html#posterior-predictive-checks"><i class="fa fa-check"></i>Posterior Predictive Checks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>2</b> Time-Series Models</a><ul>
<li class="chapter" data-level="2.1" data-path="autoregressive-section.html"><a href="autoregressive-section.html"><i class="fa fa-check"></i><b>2.1</b> Autoregressive Models</a><ul>
<li class="chapter" data-level="" data-path="autoregressive-section.html"><a href="autoregressive-section.html#ar1-models"><i class="fa fa-check"></i>AR(1) Models</a></li>
<li class="chapter" data-level="" data-path="autoregressive-section.html"><a href="autoregressive-section.html#extensions-to-the-ar1-model"><i class="fa fa-check"></i>Extensions to the AR(1) Model</a></li>
<li class="chapter" data-level="" data-path="autoregressive-section.html"><a href="autoregressive-section.html#ar2-models"><i class="fa fa-check"></i>AR(2) Models</a></li>
<li><a href="autoregressive-section.html#ark-models">AR(<span class="math inline">\(K\)</span>) Models</a></li>
<li class="chapter" data-level="" data-path="autoregressive-section.html"><a href="autoregressive-section.html#arch1-models"><i class="fa fa-check"></i>ARCH(1) Models</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="modeling-temporal-heteroscedasticity.html"><a href="modeling-temporal-heteroscedasticity.html"><i class="fa fa-check"></i><b>2.2</b> Modeling Temporal Heteroscedasticity</a><ul>
<li class="chapter" data-level="" data-path="modeling-temporal-heteroscedasticity.html"><a href="modeling-temporal-heteroscedasticity.html#garch11-models"><i class="fa fa-check"></i>GARCH(1,1) Models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="moving-average-models.html"><a href="moving-average-models.html"><i class="fa fa-check"></i><b>2.3</b> Moving Average Models</a><ul>
<li class="chapter" data-level="" data-path="moving-average-models.html"><a href="moving-average-models.html#ma2-example"><i class="fa fa-check"></i>MA(2) Example</a></li>
<li class="chapter" data-level="" data-path="moving-average-models.html"><a href="moving-average-models.html#vectorized-maq-model"><i class="fa fa-check"></i>Vectorized MA(Q) Model</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="autoregressive-moving-average-models.html"><a href="autoregressive-moving-average-models.html"><i class="fa fa-check"></i><b>2.4</b> Autoregressive Moving Average Models</a><ul>
<li class="chapter" data-level="" data-path="autoregressive-moving-average-models.html"><a href="autoregressive-moving-average-models.html#identifiability-and-stationarity"><i class="fa fa-check"></i>Identifiability and Stationarity</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="stochastic-volatility-models.html"><a href="stochastic-volatility-models.html"><i class="fa fa-check"></i><b>2.5</b> Stochastic Volatility Models</a></li>
<li class="chapter" data-level="2.6" data-path="hmms-section.html"><a href="hmms-section.html"><i class="fa fa-check"></i><b>2.6</b> Hidden Markov Models</a><ul>
<li class="chapter" data-level="" data-path="hmms-section.html"><a href="hmms-section.html#supervised-parameter-estimation"><i class="fa fa-check"></i>Supervised Parameter Estimation</a></li>
<li class="chapter" data-level="" data-path="hmms-section.html"><a href="hmms-section.html#start-state-and-end-state-probabilities"><i class="fa fa-check"></i>Start-State and End-State Probabilities</a></li>
<li class="chapter" data-level="" data-path="hmms-section.html"><a href="hmms-section.html#calculating-sufficient-statistics"><i class="fa fa-check"></i>Calculating Sufficient Statistics</a></li>
<li class="chapter" data-level="" data-path="hmms-section.html"><a href="hmms-section.html#analytic-posterior"><i class="fa fa-check"></i>Analytic Posterior</a></li>
<li class="chapter" data-level="" data-path="hmms-section.html"><a href="hmms-section.html#semisupervised-estimation"><i class="fa fa-check"></i>Semisupervised Estimation</a></li>
<li class="chapter" data-level="" data-path="hmms-section.html"><a href="hmms-section.html#predictive-inference"><i class="fa fa-check"></i>Predictive Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>3</b> Missing Data and Partially Known Parameters</a><ul>
<li class="chapter" data-level="3.1" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>3.1</b> Missing Data</a></li>
<li class="chapter" data-level="3.2" data-path="partially-known-parameters-section.html"><a href="partially-known-parameters-section.html"><i class="fa fa-check"></i><b>3.2</b> Partially Known Parameters</a></li>
<li class="chapter" data-level="3.3" data-path="sliced-missing-data.html"><a href="sliced-missing-data.html"><i class="fa fa-check"></i><b>3.3</b> Sliced Missing Data</a></li>
<li class="chapter" data-level="3.4" data-path="loading-matrix-for-factor-analysis.html"><a href="loading-matrix-for-factor-analysis.html"><i class="fa fa-check"></i><b>3.4</b> Loading matrix for factor analysis</a></li>
<li class="chapter" data-level="3.5" data-path="missing-multivariate-data.html"><a href="missing-multivariate-data.html"><i class="fa fa-check"></i><b>3.5</b> Missing Multivariate Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>4</b> Truncated or Censored Data</a><ul>
<li class="chapter" data-level="4.1" data-path="truncation-section.html"><a href="truncation-section.html"><i class="fa fa-check"></i><b>4.1</b> Truncated Distributions</a></li>
<li class="chapter" data-level="4.2" data-path="truncated-data-section.html"><a href="truncated-data-section.html"><i class="fa fa-check"></i><b>4.2</b> Truncated Data</a><ul>
<li class="chapter" data-level="" data-path="truncated-data-section.html"><a href="truncated-data-section.html#constraints-and-out-of-bounds-returns"><i class="fa fa-check"></i>Constraints and Out-of-Bounds Returns</a></li>
<li class="chapter" data-level="" data-path="truncated-data-section.html"><a href="truncated-data-section.html#unknown-truncation-points"><i class="fa fa-check"></i>Unknown Truncation Points</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="censored-data.html"><a href="censored-data.html"><i class="fa fa-check"></i><b>4.3</b> Censored Data</a><ul>
<li class="chapter" data-level="" data-path="censored-data.html"><a href="censored-data.html#estimating-censored-values"><i class="fa fa-check"></i>Estimating Censored Values</a></li>
<li class="chapter" data-level="" data-path="censored-data.html"><a href="censored-data.html#integrating-out-censored-values"><i class="fa fa-check"></i>Integrating out Censored Values</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>5</b> Finite Mixtures</a><ul>
<li class="chapter" data-level="5.1" data-path="clustering-mixture-section.html"><a href="clustering-mixture-section.html"><i class="fa fa-check"></i><b>5.1</b> Relation to Clustering</a></li>
<li class="chapter" data-level="5.2" data-path="latent-discrete-parameterization.html"><a href="latent-discrete-parameterization.html"><i class="fa fa-check"></i><b>5.2</b> Latent Discrete Parameterization</a></li>
<li class="chapter" data-level="5.3" data-path="summing-out-the-responsibility-parameter.html"><a href="summing-out-the-responsibility-parameter.html"><i class="fa fa-check"></i><b>5.3</b> Summing out the Responsibility Parameter</a><ul>
<li class="chapter" data-level="" data-path="summing-out-the-responsibility-parameter.html"><a href="summing-out-the-responsibility-parameter.html#log-sum-of-exponentials-linear-sums-on-the-log-scale"><i class="fa fa-check"></i>Log Sum of Exponentials: Linear Sums on the Log Scale</a></li>
<li class="chapter" data-level="" data-path="summing-out-the-responsibility-parameter.html"><a href="summing-out-the-responsibility-parameter.html#dropping-uniform-mixture-ratios"><i class="fa fa-check"></i>Dropping uniform mixture ratios</a></li>
<li class="chapter" data-level="" data-path="summing-out-the-responsibility-parameter.html"><a href="summing-out-the-responsibility-parameter.html#recovering-posterior-mixture-proportions"><i class="fa fa-check"></i>Recovering posterior mixture proportions</a></li>
<li class="chapter" data-level="" data-path="summing-out-the-responsibility-parameter.html"><a href="summing-out-the-responsibility-parameter.html#estimating-parameters-of-a-mixture"><i class="fa fa-check"></i>Estimating Parameters of a Mixture</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="vectorizing-mixtures.html"><a href="vectorizing-mixtures.html"><i class="fa fa-check"></i><b>5.4</b> Vectorizing Mixtures</a></li>
<li class="chapter" data-level="5.5" data-path="mixture-inference-section.html"><a href="mixture-inference-section.html"><i class="fa fa-check"></i><b>5.5</b> Inferences Supported by Mixtures</a><ul>
<li class="chapter" data-level="" data-path="mixture-inference-section.html"><a href="mixture-inference-section.html#mixtures-with-unidentifiable-components"><i class="fa fa-check"></i>Mixtures with Unidentifiable Components</a></li>
<li class="chapter" data-level="" data-path="mixture-inference-section.html"><a href="mixture-inference-section.html#inference-under-label-switching"><i class="fa fa-check"></i>Inference under Label Switching</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="zero-inflated-section.html"><a href="zero-inflated-section.html"><i class="fa fa-check"></i><b>5.6</b> Zero-Inflated and Hurdle Models</a><ul>
<li class="chapter" data-level="" data-path="zero-inflated-section.html"><a href="zero-inflated-section.html#zero-inflation"><i class="fa fa-check"></i>Zero Inflation</a></li>
<li class="chapter" data-level="" data-path="zero-inflated-section.html"><a href="zero-inflated-section.html#hurdle-models"><i class="fa fa-check"></i>Hurdle Models</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="priors-and-effective-data-size-in-mixture-models.html"><a href="priors-and-effective-data-size-in-mixture-models.html"><i class="fa fa-check"></i><b>5.7</b> Priors and Effective Data Size in Mixture Models</a><ul>
<li class="chapter" data-level="" data-path="priors-and-effective-data-size-in-mixture-models.html"><a href="priors-and-effective-data-size-in-mixture-models.html#comparison-to-model-averaging"><i class="fa fa-check"></i>Comparison to Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>6</b> Measurement Error and Meta-Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="bayesian-measurement-error-model.html"><a href="bayesian-measurement-error-model.html"><i class="fa fa-check"></i><b>6.1</b> Bayesian Measurement Error Model</a><ul>
<li class="chapter" data-level="" data-path="bayesian-measurement-error-model.html"><a href="bayesian-measurement-error-model.html#regression-with-measurement-error"><i class="fa fa-check"></i>Regression with Measurement Error</a></li>
<li class="chapter" data-level="" data-path="bayesian-measurement-error-model.html"><a href="bayesian-measurement-error-model.html#rounding"><i class="fa fa-check"></i>Rounding</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>6.2</b> Meta-Analysis</a><ul>
<li class="chapter" data-level="" data-path="meta-analysis.html"><a href="meta-analysis.html#treatment-effects-in-controlled-studies"><i class="fa fa-check"></i>Treatment Effects in Controlled Studies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>7</b> Latent Discrete Parameters</a><ul>
<li class="chapter" data-level="7.1" data-path="rao-blackwell-section.html"><a href="rao-blackwell-section.html"><i class="fa fa-check"></i><b>7.1</b> The Benefits of Marginalization</a></li>
<li class="chapter" data-level="7.2" data-path="change-point-section.html"><a href="change-point-section.html"><i class="fa fa-check"></i><b>7.2</b> Change Point Models</a><ul>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#model-with-latent-discrete-parameter"><i class="fa fa-check"></i>Model with Latent Discrete Parameter</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#marginalizing-out-the-discrete-parameter"><i class="fa fa-check"></i>Marginalizing out the Discrete Parameter</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#coding-the-model-in-stan-1"><i class="fa fa-check"></i>Coding the Model in Stan</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#fitting-the-model-with-mcmc"><i class="fa fa-check"></i>Fitting the Model with MCMC</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#posterior-distribution-of-the-discrete-change-point"><i class="fa fa-check"></i>Posterior Distribution of the Discrete Change Point</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#discrete-sampling"><i class="fa fa-check"></i>Discrete Sampling</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#posterior-covariance"><i class="fa fa-check"></i>Posterior Covariance</a></li>
<li class="chapter" data-level="" data-path="change-point-section.html"><a href="change-point-section.html#multiple-change-points"><i class="fa fa-check"></i>Multiple Change Points</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="mark-recapture-models.html"><a href="mark-recapture-models.html"><i class="fa fa-check"></i><b>7.3</b> Mark-Recapture Models</a><ul>
<li class="chapter" data-level="" data-path="mark-recapture-models.html"><a href="mark-recapture-models.html#simple-mark-recapture-model"><i class="fa fa-check"></i>Simple Mark-Recapture Model</a></li>
<li class="chapter" data-level="" data-path="mark-recapture-models.html"><a href="mark-recapture-models.html#cormack-jolly-seber-with-discrete-parameter"><i class="fa fa-check"></i>Cormack-Jolly-Seber with Discrete Parameter</a></li>
<li class="chapter" data-level="" data-path="mark-recapture-models.html"><a href="mark-recapture-models.html#collective-cormack-jolly-seber-model"><i class="fa fa-check"></i>Collective Cormack-Jolly-Seber Model</a></li>
<li class="chapter" data-level="" data-path="mark-recapture-models.html"><a href="mark-recapture-models.html#individual-cormack-jolly-seber-model"><i class="fa fa-check"></i>Individual Cormack-Jolly-Seber Model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html"><i class="fa fa-check"></i><b>7.4</b> Data Coding and Diagnostic Accuracy Models</a><ul>
<li class="chapter" data-level="" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html#diagnostic-accuracy"><i class="fa fa-check"></i>Diagnostic Accuracy</a></li>
<li class="chapter" data-level="" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html#data-coding"><i class="fa fa-check"></i>Data Coding</a></li>
<li class="chapter" data-level="" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html#noisy-categorical-measurement-model"><i class="fa fa-check"></i>Noisy Categorical Measurement Model</a></li>
<li class="chapter" data-level="" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html#model-parameters"><i class="fa fa-check"></i>Model Parameters</a></li>
<li class="chapter" data-level="" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html#noisy-measurement-model"><i class="fa fa-check"></i>Noisy Measurement Model</a></li>
<li class="chapter" data-level="" data-path="data-coding-and-diagnostic-accuracy-models.html"><a href="data-coding-and-diagnostic-accuracy-models.html#stan-implementation"><i class="fa fa-check"></i>Stan Implementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>8</b> Sparse and Ragged Data Structures</a><ul>
<li class="chapter" data-level="8.1" data-path="sparse-data-structures.html"><a href="sparse-data-structures.html"><i class="fa fa-check"></i><b>8.1</b> Sparse Data Structures</a></li>
<li class="chapter" data-level="8.2" data-path="ragged-data-structs-section.html"><a href="ragged-data-structs-section.html"><i class="fa fa-check"></i><b>8.2</b> Ragged Data Structures</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>9</b> Clustering Models</a><ul>
<li class="chapter" data-level="9.1" data-path="relation-to-finite-mixture-models.html"><a href="relation-to-finite-mixture-models.html"><i class="fa fa-check"></i><b>9.1</b> Relation to Finite Mixture Models</a></li>
<li class="chapter" data-level="9.2" data-path="soft-k-means.html"><a href="soft-k-means.html"><i class="fa fa-check"></i><b>9.2</b> Soft <span class="math inline">\(K\)</span>-Means</a><ul>
<li><a href="soft-k-means.html#geometric-hard-k-means-clustering">Geometric Hard <span class="math inline">\(K\)</span>-Means Clustering</a></li>
<li><a href="soft-k-means.html#soft-k-means-clustering">Soft <span class="math inline">\(K\)</span>-Means Clustering</a></li>
<li><a href="soft-k-means.html#stan-implementation-of-soft-k-means">Stan Implementation of Soft <span class="math inline">\(K\)</span>-Means</a></li>
<li><a href="soft-k-means.html#generalizing-soft-k-means">Generalizing Soft <span class="math inline">\(K\)</span>-Means</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="the-difficulty-of-bayesian-inference-for-clustering.html"><a href="the-difficulty-of-bayesian-inference-for-clustering.html"><i class="fa fa-check"></i><b>9.3</b> The Difficulty of Bayesian Inference for Clustering</a><ul>
<li class="chapter" data-level="" data-path="the-difficulty-of-bayesian-inference-for-clustering.html"><a href="the-difficulty-of-bayesian-inference-for-clustering.html#non-identifiability"><i class="fa fa-check"></i>Non-Identifiability</a></li>
<li class="chapter" data-level="" data-path="the-difficulty-of-bayesian-inference-for-clustering.html"><a href="the-difficulty-of-bayesian-inference-for-clustering.html#multimodality"><i class="fa fa-check"></i>Multimodality</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="naive-bayes-classification-and-clustering.html"><a href="naive-bayes-classification-and-clustering.html"><i class="fa fa-check"></i><b>9.4</b> Naive Bayes Classification and Clustering</a><ul>
<li class="chapter" data-level="" data-path="naive-bayes-classification-and-clustering.html"><a href="naive-bayes-classification-and-clustering.html#coding-ragged-arrays"><i class="fa fa-check"></i>Coding Ragged Arrays</a></li>
<li class="chapter" data-level="" data-path="naive-bayes-classification-and-clustering.html"><a href="naive-bayes-classification-and-clustering.html#estimation-with-category-labeled-training-data"><i class="fa fa-check"></i>Estimation with Category-Labeled Training Data</a></li>
<li class="chapter" data-level="" data-path="naive-bayes-classification-and-clustering.html"><a href="naive-bayes-classification-and-clustering.html#estimation-without-category-labeled-training-data"><i class="fa fa-check"></i>Estimation without Category-Labeled Training Data</a></li>
<li class="chapter" data-level="" data-path="naive-bayes-classification-and-clustering.html"><a href="naive-bayes-classification-and-clustering.html#full-bayesian-inference-for-naive-bayes"><i class="fa fa-check"></i>Full Bayesian Inference for Naive Bayes</a></li>
<li class="chapter" data-level="" data-path="naive-bayes-classification-and-clustering.html"><a href="naive-bayes-classification-and-clustering.html#prediction-without-model-updates"><i class="fa fa-check"></i>Prediction without Model Updates</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html"><i class="fa fa-check"></i><b>9.5</b> Latent Dirichlet Allocation</a><ul>
<li class="chapter" data-level="" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#the-lda-model"><i class="fa fa-check"></i>The LDA Model</a></li>
<li class="chapter" data-level="" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#summing-out-the-discrete-parameters"><i class="fa fa-check"></i>Summing out the Discrete Parameters</a></li>
<li class="chapter" data-level="" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#implementation-of-lda"><i class="fa fa-check"></i>Implementation of LDA</a></li>
<li class="chapter" data-level="" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#correlated-topic-model"><i class="fa fa-check"></i>Correlated Topic Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>10</b> Gaussian Processes</a><ul>
<li class="chapter" data-level="10.1" data-path="gaussian-process-regression.html"><a href="gaussian-process-regression.html"><i class="fa fa-check"></i><b>10.1</b> Gaussian Process Regression</a></li>
<li class="chapter" data-level="10.2" data-path="simulating-from-a-gaussian-process.html"><a href="simulating-from-a-gaussian-process.html"><i class="fa fa-check"></i><b>10.2</b> Simulating from a Gaussian Process</a><ul>
<li class="chapter" data-level="" data-path="simulating-from-a-gaussian-process.html"><a href="simulating-from-a-gaussian-process.html#multivariate-inputs"><i class="fa fa-check"></i>Multivariate Inputs</a></li>
<li class="chapter" data-level="" data-path="simulating-from-a-gaussian-process.html"><a href="simulating-from-a-gaussian-process.html#cholesky-factored-and-transformed-implementation"><i class="fa fa-check"></i>Cholesky Factored and Transformed Implementation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="fit-gp-section.html"><a href="fit-gp-section.html"><i class="fa fa-check"></i><b>10.3</b> Fitting a Gaussian Process</a><ul>
<li class="chapter" data-level="" data-path="fit-gp-section.html"><a href="fit-gp-section.html#gp-with-a-normal-outcome"><i class="fa fa-check"></i>GP with a normal outcome</a></li>
<li class="chapter" data-level="" data-path="fit-gp-section.html"><a href="fit-gp-section.html#discrete-outcomes-with-gaussian-processes"><i class="fa fa-check"></i>Discrete outcomes with Gaussian Processes</a></li>
<li class="chapter" data-level="" data-path="fit-gp-section.html"><a href="fit-gp-section.html#automatic-relevance-determination"><i class="fa fa-check"></i>Automatic Relevance Determination</a></li>
<li class="chapter" data-level="10.3.1" data-path="fit-gp-section.html"><a href="fit-gp-section.html#priors-gp.section"><i class="fa fa-check"></i><b>10.3.1</b> Priors for Gaussian Process Parameters</a></li>
<li class="chapter" data-level="" data-path="fit-gp-section.html"><a href="fit-gp-section.html#predictive-inference-with-a-gaussian-process"><i class="fa fa-check"></i>Predictive Inference with a Gaussian Process</a></li>
<li class="chapter" data-level="" data-path="fit-gp-section.html"><a href="fit-gp-section.html#multiple-output-gaussian-processes"><i class="fa fa-check"></i>Multiple-output Gaussian processes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>11</b> Directions, Rotations, and Hyperspheres</a><ul>
<li class="chapter" data-level="11.1" data-path="unit-vectors.html"><a href="unit-vectors.html"><i class="fa fa-check"></i><b>11.1</b> Unit Vectors</a></li>
<li class="chapter" data-level="11.2" data-path="circles-spheres-and-hyperspheres.html"><a href="circles-spheres-and-hyperspheres.html"><i class="fa fa-check"></i><b>11.2</b> Circles, Spheres, and Hyperspheres</a></li>
<li class="chapter" data-level="11.3" data-path="transforming-to-unconstrained-parameters.html"><a href="transforming-to-unconstrained-parameters.html"><i class="fa fa-check"></i><b>11.3</b> Transforming to Unconstrained Parameters</a></li>
<li class="chapter" data-level="11.4" data-path="unit-vectors-and-rotations.html"><a href="unit-vectors-and-rotations.html"><i class="fa fa-check"></i><b>11.4</b> Unit Vectors and Rotations</a><ul>
<li class="chapter" data-level="" data-path="unit-vectors-and-rotations.html"><a href="unit-vectors-and-rotations.html#unit-vector-type"><i class="fa fa-check"></i>Unit vector type</a></li>
<li class="chapter" data-level="" data-path="unit-vectors-and-rotations.html"><a href="unit-vectors-and-rotations.html#angles-from-unit-vectors"><i class="fa fa-check"></i>Angles from unit vectors</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="circular-representations-of-days-and-years.html"><a href="circular-representations-of-days-and-years.html"><i class="fa fa-check"></i><b>11.5</b> Circular Representations of Days and Years</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>12</b> Solving Algebraic Equations</a><ul>
<li class="chapter" data-level="12.1" data-path="example-system-of-nonlinear-algebraic-equations.html"><a href="example-system-of-nonlinear-algebraic-equations.html"><i class="fa fa-check"></i><b>12.1</b> Example: System of Nonlinear Algebraic Equations</a></li>
<li class="chapter" data-level="12.2" data-path="coding-an-algebraic-system.html"><a href="coding-an-algebraic-system.html"><i class="fa fa-check"></i><b>12.2</b> Coding an Algebraic System</a></li>
<li class="chapter" data-level="12.3" data-path="calling-the-algebraic-solver.html"><a href="calling-the-algebraic-solver.html"><i class="fa fa-check"></i><b>12.3</b> Calling the Algebraic Solver</a><ul>
<li class="chapter" data-level="" data-path="calling-the-algebraic-solver.html"><a href="calling-the-algebraic-solver.html#data-versus-parameters"><i class="fa fa-check"></i>Data versus Parameters</a></li>
<li class="chapter" data-level="" data-path="calling-the-algebraic-solver.html"><a href="calling-the-algebraic-solver.html#length-of-the-algebraic-function-and-of-the-vector-of-unknowns"><i class="fa fa-check"></i>Length of the Algebraic Function and of the Vector of Unknowns</a></li>
<li class="chapter" data-level="" data-path="calling-the-algebraic-solver.html"><a href="calling-the-algebraic-solver.html#pathological-solutions"><i class="fa fa-check"></i>Pathological Solutions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="algebra-control-section.html"><a href="algebra-control-section.html"><i class="fa fa-check"></i><b>12.4</b> Control Parameters for the Algebraic Solver</a><ul>
<li class="chapter" data-level="" data-path="algebra-control-section.html"><a href="algebra-control-section.html#tolerance"><i class="fa fa-check"></i>Tolerance</a></li>
<li class="chapter" data-level="" data-path="algebra-control-section.html"><a href="algebra-control-section.html#maximum-number-of-steps"><i class="fa fa-check"></i>Maximum Number of Steps</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>13</b> Ordinary Differential Equations</a><ul>
<li class="chapter" data-level="13.1" data-path="example-simple-harmonic-oscillator.html"><a href="example-simple-harmonic-oscillator.html"><i class="fa fa-check"></i><b>13.1</b> Example: Simple Harmonic Oscillator</a><ul>
<li class="chapter" data-level="" data-path="example-simple-harmonic-oscillator.html"><a href="example-simple-harmonic-oscillator.html#solutions-given-initial-conditions"><i class="fa fa-check"></i>Solutions Given Initial Conditions</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="coding-an-ode-system.html"><a href="coding-an-ode-system.html"><i class="fa fa-check"></i><b>13.2</b> Coding an ODE System</a><ul>
<li class="chapter" data-level="" data-path="coding-an-ode-system.html"><a href="coding-an-ode-system.html#strict-signature-1"><i class="fa fa-check"></i>Strict Signature</a></li>
<li class="chapter" data-level="" data-path="coding-an-ode-system.html"><a href="coding-an-ode-system.html#discontinuous-ode-system-function"><i class="fa fa-check"></i>Discontinuous ODE System Function</a></li>
<li class="chapter" data-level="" data-path="coding-an-ode-system.html"><a href="coding-an-ode-system.html#varying-initial-time"><i class="fa fa-check"></i>Varying Initial Time</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="solving-a-system-of-linear-odes-using-a-matrix-exponential.html"><a href="solving-a-system-of-linear-odes-using-a-matrix-exponential.html"><i class="fa fa-check"></i><b>13.3</b> Solving a System of Linear ODEs using a Matrix Exponential</a></li>
<li class="chapter" data-level="13.4" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>13.4</b> Measurement Error Models</a><ul>
<li class="chapter" data-level="" data-path="measurement-error-models.html"><a href="measurement-error-models.html#simulating-noisy-measurements"><i class="fa fa-check"></i>Simulating Noisy Measurements</a></li>
<li class="chapter" data-level="" data-path="measurement-error-models.html"><a href="measurement-error-models.html#data-versus-parameters-1"><i class="fa fa-check"></i>Data versus Parameters</a></li>
<li class="chapter" data-level="" data-path="measurement-error-models.html"><a href="measurement-error-models.html#estimating-system-parameters-and-initial-state"><i class="fa fa-check"></i>Estimating System Parameters and Initial State</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="stiff-ode-section.html"><a href="stiff-ode-section.html"><i class="fa fa-check"></i><b>13.5</b> Stiff ODEs</a></li>
<li class="chapter" data-level="13.6" data-path="control-parameters-for-ode-solving.html"><a href="control-parameters-for-ode-solving.html"><i class="fa fa-check"></i><b>13.6</b> Control Parameters for ODE Solving</a><ul>
<li class="chapter" data-level="" data-path="control-parameters-for-ode-solving.html"><a href="control-parameters-for-ode-solving.html#data-only-for-control-parameters"><i class="fa fa-check"></i>Data only for control parameters</a></li>
<li class="chapter" data-level="" data-path="control-parameters-for-ode-solving.html"><a href="control-parameters-for-ode-solving.html#tolerance-1"><i class="fa fa-check"></i>Tolerance</a></li>
<li class="chapter" data-level="" data-path="control-parameters-for-ode-solving.html"><a href="control-parameters-for-ode-solving.html#maximum-number-of-steps-1"><i class="fa fa-check"></i>Maximum Number of Steps</a></li>
</ul></li>
</ul></li>
<li><a href="part-2-programming-techniques.html#part-2.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 2. Programming Techniques</i></a></li>
<li class="chapter" data-level="14" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>14</b> Floating Point Arithmetic</a><ul>
<li class="chapter" data-level="14.1" data-path="floating-point-representations.html"><a href="floating-point-representations.html"><i class="fa fa-check"></i><b>14.1</b> Floating-point representations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="floating-point-representations.html"><a href="floating-point-representations.html#finite-values"><i class="fa fa-check"></i><b>14.1.1</b> Finite values</a></li>
<li class="chapter" data-level="14.1.2" data-path="floating-point-representations.html"><a href="floating-point-representations.html#normality"><i class="fa fa-check"></i><b>14.1.2</b> Normality</a></li>
<li class="chapter" data-level="14.1.3" data-path="floating-point-representations.html"><a href="floating-point-representations.html#ranges-and-extreme-values"><i class="fa fa-check"></i><b>14.1.3</b> Ranges and extreme values</a></li>
<li class="chapter" data-level="14.1.4" data-path="floating-point-representations.html"><a href="floating-point-representations.html#signed-zero"><i class="fa fa-check"></i><b>14.1.4</b> Signed zero</a></li>
<li class="chapter" data-level="14.1.5" data-path="floating-point-representations.html"><a href="floating-point-representations.html#not-a-number-values"><i class="fa fa-check"></i><b>14.1.5</b> Not-a-number values</a></li>
<li class="chapter" data-level="14.1.6" data-path="floating-point-representations.html"><a href="floating-point-representations.html#positive-and-negative-infinity"><i class="fa fa-check"></i><b>14.1.6</b> Positive and negative infinity</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="literals-decimal-and-scientific-notation.html"><a href="literals-decimal-and-scientific-notation.html"><i class="fa fa-check"></i><b>14.2</b> Literals: decimal and scientific notation</a></li>
<li class="chapter" data-level="14.3" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html"><i class="fa fa-check"></i><b>14.3</b> Arithmetic Precision</a><ul>
<li class="chapter" data-level="14.3.1" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#rounding-and-probabilities"><i class="fa fa-check"></i><b>14.3.1</b> Rounding and probabilities</a></li>
<li class="chapter" data-level="14.3.2" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#machine-precision-and-the-asymmetry-of-0-and-1"><i class="fa fa-check"></i><b>14.3.2</b> Machine precision and the asymmetry of 0 and 1</a></li>
<li class="chapter" data-level="14.3.3" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#complementary-and-epsilon-functions"><i class="fa fa-check"></i><b>14.3.3</b> Complementary and epsilon functions</a></li>
<li class="chapter" data-level="14.3.4" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#catastrophic-cancellation"><i class="fa fa-check"></i><b>14.3.4</b> Catastrophic cancellation</a></li>
<li class="chapter" data-level="14.3.5" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#overflow"><i class="fa fa-check"></i><b>14.3.5</b> Overflow</a></li>
<li class="chapter" data-level="14.3.6" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#underflow-and-the-log-scale"><i class="fa fa-check"></i><b>14.3.6</b> Underflow and the log scale</a></li>
<li class="chapter" data-level="14.3.7" data-path="arithmetic-precision.html"><a href="arithmetic-precision.html#adding-on-the-log-scale"><i class="fa fa-check"></i><b>14.3.7</b> Adding on the log scale</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="comparing-floating-point-numbers.html"><a href="comparing-floating-point-numbers.html"><i class="fa fa-check"></i><b>14.4</b> Comparing floating-point numbers</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>15</b> Matrices, Vectors, and Arrays</a><ul>
<li class="chapter" data-level="15.1" data-path="basic-motivation.html"><a href="basic-motivation.html"><i class="fa fa-check"></i><b>15.1</b> Basic Motivation</a></li>
<li class="chapter" data-level="15.2" data-path="fixed-sizes-and-indexing-out-of-bounds.html"><a href="fixed-sizes-and-indexing-out-of-bounds.html"><i class="fa fa-check"></i><b>15.2</b> Fixed Sizes and Indexing out of Bounds</a></li>
<li class="chapter" data-level="15.3" data-path="indexing-efficiency-section.html"><a href="indexing-efficiency-section.html"><i class="fa fa-check"></i><b>15.3</b> Data Type and Indexing Efficiency</a><ul>
<li class="chapter" data-level="" data-path="indexing-efficiency-section.html"><a href="indexing-efficiency-section.html#matrices-vs.two-dimensional-arrays"><i class="fa fa-check"></i>Matrices vs. Two-Dimensional Arrays</a></li>
<li class="chapter" data-level="" data-path="indexing-efficiency-section.html"><a href="indexing-efficiency-section.html#row-vectors-vs.one-dimensional-arrays"><i class="fa fa-check"></i>(Row) Vectors vs. One-Dimensional Arrays</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="memory-locality.html"><a href="memory-locality.html"><i class="fa fa-check"></i><b>15.4</b> Memory Locality</a><ul>
<li class="chapter" data-level="" data-path="memory-locality.html"><a href="memory-locality.html#memory-locality-1"><i class="fa fa-check"></i>Memory Locality</a></li>
<li class="chapter" data-level="" data-path="memory-locality.html"><a href="memory-locality.html#matrices"><i class="fa fa-check"></i>Matrices</a></li>
<li class="chapter" data-level="" data-path="memory-locality.html"><a href="memory-locality.html#arrays"><i class="fa fa-check"></i>Arrays</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="converting-among-matrix-vector-and-array-types.html"><a href="converting-among-matrix-vector-and-array-types.html"><i class="fa fa-check"></i><b>15.5</b> Converting among Matrix, Vector, and Array Types</a></li>
<li class="chapter" data-level="15.6" data-path="aliasing-in-stan-containers.html"><a href="aliasing-in-stan-containers.html"><i class="fa fa-check"></i><b>15.6</b> Aliasing in Stan Containers</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>16</b> Multiple Indexing and Range Indexing</a><ul>
<li class="chapter" data-level="16.1" data-path="multiple-indexing.html"><a href="multiple-indexing.html"><i class="fa fa-check"></i><b>16.1</b> Multiple Indexing</a></li>
<li class="chapter" data-level="16.2" data-path="slicing-with-range-indexes.html"><a href="slicing-with-range-indexes.html"><i class="fa fa-check"></i><b>16.2</b> Slicing with Range Indexes</a><ul>
<li class="chapter" data-level="" data-path="slicing-with-range-indexes.html"><a href="slicing-with-range-indexes.html#lower-and-upper-bound-indexes"><i class="fa fa-check"></i>Lower and Upper Bound Indexes</a></li>
<li class="chapter" data-level="" data-path="slicing-with-range-indexes.html"><a href="slicing-with-range-indexes.html#lower-or-upper-bound-indexes"><i class="fa fa-check"></i>Lower or Upper Bound Indexes</a></li>
<li class="chapter" data-level="" data-path="slicing-with-range-indexes.html"><a href="slicing-with-range-indexes.html#full-range-indexes"><i class="fa fa-check"></i>Full Range Indexes</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="multiple-indexing-on-the-left-of-assignments.html"><a href="multiple-indexing-on-the-left-of-assignments.html"><i class="fa fa-check"></i><b>16.3</b> Multiple Indexing on the Left of Assignments</a><ul>
<li class="chapter" data-level="" data-path="multiple-indexing-on-the-left-of-assignments.html"><a href="multiple-indexing-on-the-left-of-assignments.html#assign-by-value-and-aliasing"><i class="fa fa-check"></i>Assign-by-Value and Aliasing</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="multiple-indexes-with-vectors-and-matrices.html"><a href="multiple-indexes-with-vectors-and-matrices.html"><i class="fa fa-check"></i><b>16.4</b> Multiple Indexes with Vectors and Matrices</a><ul>
<li class="chapter" data-level="" data-path="multiple-indexes-with-vectors-and-matrices.html"><a href="multiple-indexes-with-vectors-and-matrices.html#vectors"><i class="fa fa-check"></i>Vectors</a></li>
<li class="chapter" data-level="" data-path="multiple-indexes-with-vectors-and-matrices.html"><a href="multiple-indexes-with-vectors-and-matrices.html#matrices-1"><i class="fa fa-check"></i>Matrices</a></li>
<li class="chapter" data-level="" data-path="multiple-indexes-with-vectors-and-matrices.html"><a href="multiple-indexes-with-vectors-and-matrices.html#matrices-with-one-multiple-index"><i class="fa fa-check"></i>Matrices with One Multiple Index</a></li>
<li class="chapter" data-level="" data-path="multiple-indexes-with-vectors-and-matrices.html"><a href="multiple-indexes-with-vectors-and-matrices.html#arrays-of-vectors-or-matrices"><i class="fa fa-check"></i>Arrays of Vectors or Matrices</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="matrices-with-parameters-and-constants.html"><a href="matrices-with-parameters-and-constants.html"><i class="fa fa-check"></i><b>16.5</b> Matrices with Parameters and Constants</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>17</b> User-Defined Functions</a><ul>
<li class="chapter" data-level="17.1" data-path="basic-functions-section.html"><a href="basic-functions-section.html"><i class="fa fa-check"></i><b>17.1</b> Basic Functions</a><ul>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#user-defined-functions-block"><i class="fa fa-check"></i>User-Defined Functions Block</a></li>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#function-bodies"><i class="fa fa-check"></i>Function Bodies</a></li>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#return-statements"><i class="fa fa-check"></i>Return Statements</a></li>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#reject-statements"><i class="fa fa-check"></i>Reject Statements</a></li>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#type-declarations-for-functions"><i class="fa fa-check"></i>Type Declarations for Functions</a></li>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#array-types-for-function-declarations"><i class="fa fa-check"></i>Array Types for Function Declarations</a></li>
<li class="chapter" data-level="" data-path="basic-functions-section.html"><a href="basic-functions-section.html#data-only-function-arguments"><i class="fa fa-check"></i>Data-only Function Arguments</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="functions-as-statements.html"><a href="functions-as-statements.html"><i class="fa fa-check"></i><b>17.2</b> Functions as Statements</a></li>
<li class="chapter" data-level="17.3" data-path="functions-accessing-the-log-probability-accumulator.html"><a href="functions-accessing-the-log-probability-accumulator.html"><i class="fa fa-check"></i><b>17.3</b> Functions Accessing the Log Probability Accumulator</a></li>
<li class="chapter" data-level="17.4" data-path="functions-acting-as-random-number-generators.html"><a href="functions-acting-as-random-number-generators.html"><i class="fa fa-check"></i><b>17.4</b> Functions Acting as Random Number Generators</a></li>
<li class="chapter" data-level="17.5" data-path="user-defined-probability-functions.html"><a href="user-defined-probability-functions.html"><i class="fa fa-check"></i><b>17.5</b> User-Defined Probability Functions</a></li>
<li class="chapter" data-level="17.6" data-path="overloading-functions.html"><a href="overloading-functions.html"><i class="fa fa-check"></i><b>17.6</b> Overloading Functions</a></li>
<li class="chapter" data-level="17.7" data-path="documenting-functions-section.html"><a href="documenting-functions-section.html"><i class="fa fa-check"></i><b>17.7</b> Documenting Functions</a></li>
<li class="chapter" data-level="17.8" data-path="summary-of-function-types.html"><a href="summary-of-function-types.html"><i class="fa fa-check"></i><b>17.8</b> Summary of Function Types</a><ul>
<li class="chapter" data-level="" data-path="summary-of-function-types.html"><a href="summary-of-function-types.html#void-vs.non-void-return"><i class="fa fa-check"></i>Void vs. Non-Void Return</a></li>
<li class="chapter" data-level="" data-path="summary-of-function-types.html"><a href="summary-of-function-types.html#suffixed-or-non-suffixed"><i class="fa fa-check"></i>Suffixed or Non-Suffixed</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="recursive-functions.html"><a href="recursive-functions.html"><i class="fa fa-check"></i><b>17.9</b> Recursive Functions</a></li>
<li class="chapter" data-level="17.10" data-path="truncated-random-number-generation.html"><a href="truncated-random-number-generation.html"><i class="fa fa-check"></i><b>17.10</b> Truncated Random Number Generation</a><ul>
<li class="chapter" data-level="" data-path="truncated-random-number-generation.html"><a href="truncated-random-number-generation.html#generation-with-inverse-cdfs"><i class="fa fa-check"></i>Generation with Inverse CDFs</a></li>
<li class="chapter" data-level="" data-path="truncated-random-number-generation.html"><a href="truncated-random-number-generation.html#truncated-variate-generation"><i class="fa fa-check"></i>Truncated variate generation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>18</b> Custom Probability Functions</a><ul>
<li class="chapter" data-level="18.1" data-path="examples.html"><a href="examples.html"><i class="fa fa-check"></i><b>18.1</b> Examples</a><ul>
<li class="chapter" data-level="" data-path="examples.html"><a href="examples.html#triangle-distribution"><i class="fa fa-check"></i>Triangle distribution</a></li>
<li class="chapter" data-level="" data-path="examples.html"><a href="examples.html#exponential-distribution"><i class="fa fa-check"></i>Exponential distribution</a></li>
<li class="chapter" data-level="" data-path="examples.html"><a href="examples.html#bivariate-normal-cumulative-distribution-function"><i class="fa fa-check"></i>Bivariate normal cumulative distribution function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>19</b> Problematic Posteriors</a><ul>
<li class="chapter" data-level="19.1" data-path="collinearity-section.html"><a href="collinearity-section.html"><i class="fa fa-check"></i><b>19.1</b> Collinearity of Predictors in Regressions</a><ul>
<li class="chapter" data-level="" data-path="collinearity-section.html"><a href="collinearity-section.html#examples-of-collinearity"><i class="fa fa-check"></i>Examples of Collinearity</a></li>
<li class="chapter" data-level="" data-path="collinearity-section.html"><a href="collinearity-section.html#mitigating-the-invariances"><i class="fa fa-check"></i>Mitigating the Invariances</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="label-switching-problematic-section.html"><a href="label-switching-problematic-section.html"><i class="fa fa-check"></i><b>19.2</b> Label Switching in Mixture Models</a><ul>
<li class="chapter" data-level="" data-path="label-switching-problematic-section.html"><a href="label-switching-problematic-section.html#mixture-models"><i class="fa fa-check"></i>Mixture Models</a></li>
<li class="chapter" data-level="" data-path="label-switching-problematic-section.html"><a href="label-switching-problematic-section.html#convergence-monitoring-and-effective-sample-size"><i class="fa fa-check"></i>Convergence Monitoring and Effective Sample Size</a></li>
<li class="chapter" data-level="" data-path="label-switching-problematic-section.html"><a href="label-switching-problematic-section.html#some-inferences-are-invariant"><i class="fa fa-check"></i>Some Inferences are Invariant</a></li>
<li class="chapter" data-level="" data-path="label-switching-problematic-section.html"><a href="label-switching-problematic-section.html#highly-multimodal-posteriors"><i class="fa fa-check"></i>Highly Multimodal Posteriors</a></li>
<li class="chapter" data-level="" data-path="label-switching-problematic-section.html"><a href="label-switching-problematic-section.html#hacks-as-fixes"><i class="fa fa-check"></i>Hacks as Fixes</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="component-collapsing-in-mixture-models.html"><a href="component-collapsing-in-mixture-models.html"><i class="fa fa-check"></i><b>19.3</b> Component Collapsing in Mixture Models</a></li>
<li class="chapter" data-level="19.4" data-path="posteriors-with-unbounded-densities.html"><a href="posteriors-with-unbounded-densities.html"><i class="fa fa-check"></i><b>19.4</b> Posteriors with Unbounded Densities</a><ul>
<li class="chapter" data-level="" data-path="posteriors-with-unbounded-densities.html"><a href="posteriors-with-unbounded-densities.html#mixture-models-with-varying-scales"><i class="fa fa-check"></i>Mixture Models with Varying Scales</a></li>
<li class="chapter" data-level="" data-path="posteriors-with-unbounded-densities.html"><a href="posteriors-with-unbounded-densities.html#beta-binomial-models-with-skewed-data-and-weak-priors"><i class="fa fa-check"></i>Beta-Binomial Models with Skewed Data and Weak Priors</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="posteriors-with-unbounded-parameters.html"><a href="posteriors-with-unbounded-parameters.html"><i class="fa fa-check"></i><b>19.5</b> Posteriors with Unbounded Parameters</a><ul>
<li class="chapter" data-level="" data-path="posteriors-with-unbounded-parameters.html"><a href="posteriors-with-unbounded-parameters.html#separability-in-logistic-regression"><i class="fa fa-check"></i>Separability in Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="uniform-posteriors.html"><a href="uniform-posteriors.html"><i class="fa fa-check"></i><b>19.6</b> Uniform Posteriors</a></li>
<li class="chapter" data-level="19.7" data-path="sampling-difficulties-with-problematic-priors.html"><a href="sampling-difficulties-with-problematic-priors.html"><i class="fa fa-check"></i><b>19.7</b> Sampling Difficulties with Problematic Priors</a><ul>
<li class="chapter" data-level="" data-path="sampling-difficulties-with-problematic-priors.html"><a href="sampling-difficulties-with-problematic-priors.html#gibbs-sampling"><i class="fa fa-check"></i>Gibbs Sampling</a></li>
<li class="chapter" data-level="" data-path="sampling-difficulties-with-problematic-priors.html"><a href="sampling-difficulties-with-problematic-priors.html#hamiltonian-monte-carlo-sampling"><i class="fa fa-check"></i>Hamiltonian Monte Carlo Sampling</a></li>
<li class="chapter" data-level="" data-path="sampling-difficulties-with-problematic-priors.html"><a href="sampling-difficulties-with-problematic-priors.html#no-u-turn-sampling"><i class="fa fa-check"></i>No-U-Turn Sampling</a></li>
<li class="chapter" data-level="" data-path="sampling-difficulties-with-problematic-priors.html"><a href="sampling-difficulties-with-problematic-priors.html#examples-fits-in-stan"><i class="fa fa-check"></i>Examples: Fits in Stan</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>20</b> Reparameterization and Change of Variables</a><ul>
<li class="chapter" data-level="20.1" data-path="theoretical-and-practical-background.html"><a href="theoretical-and-practical-background.html"><i class="fa fa-check"></i><b>20.1</b> Theoretical and Practical Background</a></li>
<li class="chapter" data-level="20.2" data-path="reparameterizations.html"><a href="reparameterizations.html"><i class="fa fa-check"></i><b>20.2</b> Reparameterizations</a><ul>
<li class="chapter" data-level="" data-path="reparameterizations.html"><a href="reparameterizations.html#beta-and-dirichlet-priors"><i class="fa fa-check"></i>Beta and Dirichlet Priors</a></li>
<li class="chapter" data-level="" data-path="reparameterizations.html"><a href="reparameterizations.html#transforming-unconstrained-priors-probit-and-logit"><i class="fa fa-check"></i>Transforming Unconstrained Priors: Probit and Logit</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="changes-of-variables.html"><a href="changes-of-variables.html"><i class="fa fa-check"></i><b>20.3</b> Changes of Variables</a><ul>
<li class="chapter" data-level="" data-path="changes-of-variables.html"><a href="changes-of-variables.html#change-of-variables-vs.transformations"><i class="fa fa-check"></i>Change of Variables vs. Transformations</a></li>
<li class="chapter" data-level="" data-path="changes-of-variables.html"><a href="changes-of-variables.html#multivariate-changes-of-variables"><i class="fa fa-check"></i>Multivariate Changes of Variables</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="vectors-with-varying-bounds.html"><a href="vectors-with-varying-bounds.html"><i class="fa fa-check"></i><b>20.4</b> Vectors with Varying Bounds</a><ul>
<li class="chapter" data-level="" data-path="vectors-with-varying-bounds.html"><a href="vectors-with-varying-bounds.html#varying-lower-bounds"><i class="fa fa-check"></i>Varying Lower Bounds</a></li>
<li class="chapter" data-level="" data-path="vectors-with-varying-bounds.html"><a href="vectors-with-varying-bounds.html#varying-upper-and-lower-bounds"><i class="fa fa-check"></i>Varying Upper and Lower Bounds</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>21</b> Efficiency Tuning</a><ul>
<li class="chapter" data-level="21.1" data-path="what-is-efficiency.html"><a href="what-is-efficiency.html"><i class="fa fa-check"></i><b>21.1</b> What is Efficiency?</a></li>
<li class="chapter" data-level="21.2" data-path="efficiency-for-probabilistic-models-and-algorithms.html"><a href="efficiency-for-probabilistic-models-and-algorithms.html"><i class="fa fa-check"></i><b>21.2</b> Efficiency for Probabilistic Models and Algorithms</a></li>
<li class="chapter" data-level="21.3" data-path="statistical-vs-computational-efficiency.html"><a href="statistical-vs-computational-efficiency.html"><i class="fa fa-check"></i><b>21.3</b> Statistical vs.  Computational Efficiency</a></li>
<li class="chapter" data-level="21.4" data-path="model-conditioning-and-curvature.html"><a href="model-conditioning-and-curvature.html"><i class="fa fa-check"></i><b>21.4</b> Model Conditioning and Curvature</a><ul>
<li class="chapter" data-level="" data-path="model-conditioning-and-curvature.html"><a href="model-conditioning-and-curvature.html#condition-number-and-adaptation"><i class="fa fa-check"></i>Condition Number and Adaptation</a></li>
<li class="chapter" data-level="" data-path="model-conditioning-and-curvature.html"><a href="model-conditioning-and-curvature.html#unit-scales-without-correlation"><i class="fa fa-check"></i>Unit Scales without Correlation</a></li>
<li class="chapter" data-level="" data-path="model-conditioning-and-curvature.html"><a href="model-conditioning-and-curvature.html#varying-curvature"><i class="fa fa-check"></i>Varying Curvature</a></li>
<li class="chapter" data-level="" data-path="model-conditioning-and-curvature.html"><a href="model-conditioning-and-curvature.html#reparameterizing-with-a-change-of-variables"><i class="fa fa-check"></i>Reparameterizing with a Change of Variables</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="well-specified-models.html"><a href="well-specified-models.html"><i class="fa fa-check"></i><b>21.5</b> Well-Specified Models</a></li>
<li class="chapter" data-level="21.6" data-path="avoiding-validation.html"><a href="avoiding-validation.html"><i class="fa fa-check"></i><b>21.6</b> Avoiding Validation</a></li>
<li class="chapter" data-level="21.7" data-path="reparameterization-section.html"><a href="reparameterization-section.html"><i class="fa fa-check"></i><b>21.7</b> Reparameterization</a><ul>
<li class="chapter" data-level="" data-path="reparameterization-section.html"><a href="reparameterization-section.html#example-neals-funnel"><i class="fa fa-check"></i>Example: Neal’s Funnel</a></li>
<li class="chapter" data-level="" data-path="reparameterization-section.html"><a href="reparameterization-section.html#reparameterizing-the-cauchy"><i class="fa fa-check"></i>Reparameterizing the Cauchy</a></li>
<li class="chapter" data-level="" data-path="reparameterization-section.html"><a href="reparameterization-section.html#reparameterizing-a-student-t-distribution"><i class="fa fa-check"></i>Reparameterizing a Student-t Distribution</a></li>
<li class="chapter" data-level="" data-path="reparameterization-section.html"><a href="reparameterization-section.html#hierarchical-models-and-the-non-centered-parameterization"><i class="fa fa-check"></i>Hierarchical Models and the Non-Centered Parameterization</a></li>
<li class="chapter" data-level="" data-path="reparameterization-section.html"><a href="reparameterization-section.html#non-centered-parameterization"><i class="fa fa-check"></i>Non-Centered Parameterization</a></li>
<li class="chapter" data-level="" data-path="reparameterization-section.html"><a href="reparameterization-section.html#multivariate-reparameterizations"><i class="fa fa-check"></i>Multivariate Reparameterizations</a></li>
</ul></li>
<li class="chapter" data-level="21.8" data-path="vectorization.html"><a href="vectorization.html"><i class="fa fa-check"></i><b>21.8</b> Vectorization</a><ul>
<li class="chapter" data-level="" data-path="vectorization.html"><a href="vectorization.html#gradient-bottleneck"><i class="fa fa-check"></i>Gradient Bottleneck</a></li>
<li class="chapter" data-level="" data-path="vectorization.html"><a href="vectorization.html#vectorizing-summations"><i class="fa fa-check"></i>Vectorizing Summations</a></li>
<li class="chapter" data-level="" data-path="vectorization.html"><a href="vectorization.html#vectorization-through-matrix-operations"><i class="fa fa-check"></i>Vectorization through Matrix Operations</a></li>
<li class="chapter" data-level="" data-path="vectorization.html"><a href="vectorization.html#vectorized-probability-functions"><i class="fa fa-check"></i>Vectorized Probability Functions</a></li>
<li class="chapter" data-level="" data-path="vectorization.html"><a href="vectorization.html#reshaping-data-for-vectorization"><i class="fa fa-check"></i>Reshaping Data for Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="21.9" data-path="exploiting-sufficient-statistics.html"><a href="exploiting-sufficient-statistics.html"><i class="fa fa-check"></i><b>21.9</b> Exploiting Sufficient Statistics</a></li>
<li class="chapter" data-level="21.10" data-path="aggregating-common-subexpressions.html"><a href="aggregating-common-subexpressions.html"><i class="fa fa-check"></i><b>21.10</b> Aggregating Common Subexpressions</a></li>
<li class="chapter" data-level="21.11" data-path="exploiting-conjugacy.html"><a href="exploiting-conjugacy.html"><i class="fa fa-check"></i><b>21.11</b> Exploiting Conjugacy</a></li>
<li class="chapter" data-level="21.12" data-path="standardizing-predictors-and-outputs.html"><a href="standardizing-predictors-and-outputs.html"><i class="fa fa-check"></i><b>21.12</b> Standardizing Predictors and Outputs</a><ul>
<li class="chapter" data-level="" data-path="standardizing-predictors-and-outputs.html"><a href="standardizing-predictors-and-outputs.html#standard-normal-distribution"><i class="fa fa-check"></i>Standard Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="21.13" data-path="using-map-reduce.html"><a href="using-map-reduce.html"><i class="fa fa-check"></i><b>21.13</b> Using Map-Reduce</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>22</b> Map-Reduce</a><ul>
<li class="chapter" data-level="22.1" data-path="overview-of-map-reduce.html"><a href="overview-of-map-reduce.html"><i class="fa fa-check"></i><b>22.1</b> Overview of Map-Reduce</a></li>
<li class="chapter" data-level="22.2" data-path="map-function.html"><a href="map-function.html"><i class="fa fa-check"></i><b>22.2</b> Map Function</a><ul>
<li class="chapter" data-level="" data-path="map-function.html"><a href="map-function.html#map-function-signature"><i class="fa fa-check"></i>Map Function Signature</a></li>
<li class="chapter" data-level="" data-path="map-function.html"><a href="map-function.html#map-function-semantics"><i class="fa fa-check"></i>Map Function Semantics</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="example-mapping-logistic-regression.html"><a href="example-mapping-logistic-regression.html"><i class="fa fa-check"></i><b>22.3</b> Example: Mapping Logistic Regression</a><ul>
<li class="chapter" data-level="" data-path="example-mapping-logistic-regression.html"><a href="example-mapping-logistic-regression.html#unmapped-logistic-regression"><i class="fa fa-check"></i>Unmapped Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="example-mapping-logistic-regression.html"><a href="example-mapping-logistic-regression.html#mapped-logistic-regression"><i class="fa fa-check"></i>Mapped Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="example-hierarchical-logistic-regression.html"><a href="example-hierarchical-logistic-regression.html"><i class="fa fa-check"></i><b>22.4</b> Example: Hierarchical Logistic Regression</a><ul>
<li class="chapter" data-level="" data-path="example-hierarchical-logistic-regression.html"><a href="example-hierarchical-logistic-regression.html#unmapped-implementation"><i class="fa fa-check"></i>Unmapped Implementation</a></li>
<li class="chapter" data-level="" data-path="example-hierarchical-logistic-regression.html"><a href="example-hierarchical-logistic-regression.html#mapped-implementation"><i class="fa fa-check"></i>Mapped Implementation</a></li>
</ul></li>
<li class="chapter" data-level="22.5" data-path="ragged-inputs-and-outputs.html"><a href="ragged-inputs-and-outputs.html"><i class="fa fa-check"></i><b>22.5</b> Ragged Inputs and Outputs</a><ul>
<li class="chapter" data-level="" data-path="ragged-inputs-and-outputs.html"><a href="ragged-inputs-and-outputs.html#ragged-inputs"><i class="fa fa-check"></i>Ragged Inputs</a></li>
<li class="chapter" data-level="" data-path="ragged-inputs-and-outputs.html"><a href="ragged-inputs-and-outputs.html#ragged-outputs"><i class="fa fa-check"></i>Ragged Outputs</a></li>
</ul></li>
</ul></li>
<li><a href="appendices-part.html#appendices.part"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="23" data-path="stan-program-style-guide.html"><a href="stan-program-style-guide.html"><i class="fa fa-check"></i><b>23</b> Stan Program Style Guide</a><ul>
<li class="chapter" data-level="23.1" data-path="choose-a-consistent-style.html"><a href="choose-a-consistent-style.html"><i class="fa fa-check"></i><b>23.1</b> Choose a Consistent Style</a></li>
<li class="chapter" data-level="23.2" data-path="line-length.html"><a href="line-length.html"><i class="fa fa-check"></i><b>23.2</b> Line Length</a></li>
<li class="chapter" data-level="23.3" data-path="file-extensions.html"><a href="file-extensions.html"><i class="fa fa-check"></i><b>23.3</b> File Extensions</a></li>
<li class="chapter" data-level="23.4" data-path="variable-naming.html"><a href="variable-naming.html"><i class="fa fa-check"></i><b>23.4</b> Variable Naming</a></li>
<li class="chapter" data-level="23.5" data-path="local-variable-scope.html"><a href="local-variable-scope.html"><i class="fa fa-check"></i><b>23.5</b> Local Variable Scope</a></li>
<li class="chapter" data-level="23.6" data-path="parentheses-and-brackets.html"><a href="parentheses-and-brackets.html"><i class="fa fa-check"></i><b>23.6</b> Parentheses and Brackets</a><ul>
<li class="chapter" data-level="" data-path="parentheses-and-brackets.html"><a href="parentheses-and-brackets.html#optional-parentheses-for-single-statement-blocks"><i class="fa fa-check"></i>Optional Parentheses for Single-Statement Blocks</a></li>
<li class="chapter" data-level="" data-path="parentheses-and-brackets.html"><a href="parentheses-and-brackets.html#parentheses-in-nested-operator-expressions"><i class="fa fa-check"></i>Parentheses in Nested Operator Expressions</a></li>
<li class="chapter" data-level="" data-path="parentheses-and-brackets.html"><a href="parentheses-and-brackets.html#no-open-brackets-on-own-line"><i class="fa fa-check"></i>No Open Brackets on Own Line</a></li>
</ul></li>
<li class="chapter" data-level="23.7" data-path="conditionals.html"><a href="conditionals.html"><i class="fa fa-check"></i><b>23.7</b> Conditionals</a><ul>
<li class="chapter" data-level="" data-path="conditionals.html"><a href="conditionals.html#explicit-comparisons-of-non-boolean-conditions"><i class="fa fa-check"></i>Explicit Comparisons of Non-Boolean Conditions</a></li>
</ul></li>
<li class="chapter" data-level="23.8" data-path="functions.html"><a href="functions.html"><i class="fa fa-check"></i><b>23.8</b> Functions</a></li>
<li class="chapter" data-level="23.9" data-path="white-space.html"><a href="white-space.html"><i class="fa fa-check"></i><b>23.9</b> White Space</a><ul>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#line-breaks-between-statements-and-declarations"><i class="fa fa-check"></i>Line Breaks Between Statements and Declarations</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#no-tabs"><i class="fa fa-check"></i>No Tabs</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#two-character-indents"><i class="fa fa-check"></i>Two-Character Indents</a></li>
<li class="chapter" data-level="23.9.1" data-path="white-space.html"><a href="white-space.html#space-between-if-and-condition"><i class="fa fa-check"></i><b>23.9.1</b> Space Between <code>if</code>, <code>{</code> and Condition</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#no-space-for-function-calls"><i class="fa fa-check"></i>No Space For Function Calls</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#spaces-around-operators"><i class="fa fa-check"></i>Spaces Around Operators</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#breaking-expressions-across-lines"><i class="fa fa-check"></i>Breaking Expressions across Lines</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#optional-spaces-after-commas"><i class="fa fa-check"></i>Optional Spaces after Commas</a></li>
<li class="chapter" data-level="" data-path="white-space.html"><a href="white-space.html#unix-newlines"><i class="fa fa-check"></i>Unix Newlines</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i><b>24</b> Transitioning from BUGS</a><ul>
<li class="chapter" data-level="24.1" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html"><i class="fa fa-check"></i><b>24.1</b> Some Differences in How BUGS and Stan Work</a><ul>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#bugs-is-interpreted-stan-is-compiled"><i class="fa fa-check"></i>BUGS is interpreted, Stan is compiled</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#bugs-performs-mcmc-updating-one-scalar-parameter-at-a-time-stan-uses-hmc-which-moves-in-the-entire-space-of-all-the-parameters-at-each-step"><i class="fa fa-check"></i>BUGS performs MCMC updating one scalar parameter at a time, Stan uses HMC which moves in the entire space of all the parameters at each step</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#differences-in-tuning-during-warmup"><i class="fa fa-check"></i>Differences in tuning during warmup</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#the-stan-language-is-directly-executable-the-bugs-modeling-language-is-not"><i class="fa fa-check"></i>The Stan language is directly executable, the BUGS modeling language is not</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#differences-in-statement-order"><i class="fa fa-check"></i>Differences in statement order</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#stan-computes-the-gradient-of-the-log-density-bugs-computes-the-log-density-but-not-its-gradient"><i class="fa fa-check"></i>Stan computes the gradient of the log density, BUGS computes the log density but not its gradient</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#both-bugs-and-stan-are-semi-automatic"><i class="fa fa-check"></i>Both BUGS and Stan are semi-automatic</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#licensing"><i class="fa fa-check"></i>Licensing</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#interfaces"><i class="fa fa-check"></i>Interfaces</a></li>
<li class="chapter" data-level="" data-path="some-differences-in-how-bugs-and-stan-work.html"><a href="some-differences-in-how-bugs-and-stan-work.html#platforms"><i class="fa fa-check"></i>Platforms</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="some-differences-in-the-modeling-languages.html"><a href="some-differences-in-the-modeling-languages.html"><i class="fa fa-check"></i><b>24.2</b> Some Differences in the Modeling Languages</a></li>
<li class="chapter" data-level="24.3" data-path="some-differences-in-the-statistical-models-that-are-allowed.html"><a href="some-differences-in-the-statistical-models-that-are-allowed.html"><i class="fa fa-check"></i><b>24.3</b> Some Differences in the Statistical Models that are Allowed</a></li>
<li class="chapter" data-level="24.4" data-path="some-differences-when-running-from-r.html"><a href="some-differences-when-running-from-r.html"><i class="fa fa-check"></i><b>24.4</b> Some Differences when Running from R</a></li>
<li class="chapter" data-level="24.5" data-path="the-stan-community.html"><a href="the-stan-community.html"><i class="fa fa-check"></i><b>24.5</b> The Stan Community</a></li>
</ul></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan User’s Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fit-gp.section" class="section level2">
<h2><span class="header-section-number">10.3</span> Fitting a Gaussian Process</h2>
<div id="gp-with-a-normal-outcome" class="section level3 unnumbered">
<h3>GP with a normal outcome</h3>
<p>The full generative model for a GP with a normal outcome,
<span class="math inline">\(y \in \mathbb{R}^N\)</span>, with inputs <span class="math inline">\(x \in \mathbb{R}^N\)</span>, for a finite <span class="math inline">\(N\)</span>:</p>
<p><span class="math display">\[ \begin{aligned}
  \rho &amp; \sim \mathsf{InvGamma}(5, 5) \\
  \alpha &amp; \sim \mathsf{normal}(0, 1) \\
  \sigma &amp; \sim \mathsf{normal}(0, 1) \\
  f &amp; \sim \mathsf{multivariate\ normal}\left(0, K(x | \alpha, \rho)\right) \\
  y_i &amp; \sim \mathsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\}
\end{aligned} \]</span></p>
<p>With a normal outcome, it is possible to integrate out the Gaussian
process <span class="math inline">\(f\)</span>, yielding the more parsimonious model:</p>
<p><span class="math display">\[ \begin{aligned}
  \rho &amp; \sim \mathsf{InvGamma}(5, 5) \\
  \alpha &amp; \sim \mathsf{normal}(0, 1) \\
  \sigma &amp; \sim \mathsf{normal}(0, 1) \\
  y &amp; \sim \mathsf{multivariate\ normal}
  \left(0, K(x | \alpha, \rho) + \mathbf{I}_N \sigma^2\right) \\
\end{aligned} \]</span></p>
<p>It can be more computationally efficient when dealing with a normal
outcome to integrate out the Gaussian process, because this yields a
lower-dimensional parameter space over which to do inference. We’ll fit
both models in Stan. The former model will be referred to as the latent
variable GP, while the latter will be called the marginal likelihood
GP.</p>
<p>The hyperparameters controlling the covariance function of a Gaussian
process can be fit by assigning them priors, like we have in the
generative models above, and then computing the posterior distribution
of the hyperparameters given observed data. The priors on the
parameters should be defined based on prior knowledge of the scale of
the output values (<span class="math inline">\(\alpha\)</span>), the scale of the output noise
(<span class="math inline">\(\sigma\)</span>), and the scale at which distances are measured among inputs
(<span class="math inline">\(\rho\)</span>). See the <a href="fit-gp-section.html#priors-gp.section">Gaussian process priors section</a>
for more information about how to specify
appropriate priors for the hyperparameters.</p>
<p>The Stan program implementing the marginal likelihood GP is shown below. The
program is similar to the Stan programs that implement the simulation GPs
above, but because we are doing inference on the hyperparameters, we need to
calculate the covariance matrix <code>K</code> in the model block, rather than
the transformed data block.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real x[N];
  vector[N] y;
}
transformed data {
  vector[N] mu = rep_vector(0, N);
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
}
model {
  matrix[N, N] L_K;
  matrix[N, N] K = cov_exp_quad(x, alpha, rho);
  real sq_sigma = square(sigma);

  // diagonal elements
  for (n in 1:N)
    K[n, n] = K[n, n] + sq_sigma;

  L_K = cholesky_decompose(K);

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();

  y ~ multi_normal_cholesky(mu, L_K);
}</code></pre>
<p>The data block now declares a vector <code>y</code> of observed values <code>y[n]</code>
for inputs <code>x[n]</code>. The transformed data block now only defines the mean
vector to be zero. The three hyperparameters are defined as parameters
constrained to be non-negative. The computation of the covariance matrix
<code>K</code> is now in the model block because it involves unknown parameters and
thus can’t simply be precomputed as transformed data. The rest of the model
consists of the priors for the hyperparameters and the multivariate
Cholesky-parameterized normal likelihood, only now the value <code>y</code> is known
and the covariance matrix <code>K</code> is an unknown dependent on the
hyperparameters, allowing us to learn the hyperparameters.</p>
<p>We have used the Cholesky parameterized multivariate normal rather
than the standard parameterization because it allows us to the
<code>cholesky_decompose</code> function which has been optimized for both small
and large matrices. When working with small matrices the differences
in computational speed between the two approaches will not be
noticeable, but for larger matrices (<span class="math inline">\(N \gtrsim 100\)</span>) the Cholesky
decomposition version will be faster.</p>
<p>Hamiltonian Monte Carlo sampling is fast and effective for hyperparameter
inference in this model <span class="citation">Neal (<a href="#ref-Neal:1997">1997</a>)</span>. If the posterior is
well-concentrated for the hyperparameters the Stan implementation will fit
hyperparameters in models with a few hundred data points in seconds.</p>
<div id="latent-variable-gp" class="section level4 unnumbered">
<h4>Latent variable GP</h4>
<p>We can also explicitly code the latent variable formulation of a GP in Stan.
This will be useful for when the outcome is not normal. We’ll need to add a
small positive term, <span class="math inline">\(\delta\)</span> to the diagonal of the covariance matrix in order
to ensure that our covariance matrix remains positive definite.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}</code></pre>
<p>Two differences between the latent variable GP and the marginal likelihood GP
are worth noting. The first is that we have augmented our parameter block with
a new parameter vector of length <span class="math inline">\(N\)</span> called <span class="math inline">\(`eta`\)</span>. This is used in the model
block to generate a multivariate normal vector called <span class="math inline">\(f\)</span>, corresponding to the
latent GP. We put a <span class="math inline">\(\mathsf{normal}(0,1)\)</span> prior on <code>eta</code> like we did in the
Cholesky-parameterized GP in the simulation section. The second difference is
that our likelihood is now univariate, though we could code <span class="math inline">\(N\)</span> likelihood
terms as one <span class="math inline">\(N\)</span>-dimensional multivariate normal with an identity covariance
matrix multiplied by <span class="math inline">\(\sigma^2\)</span>. However, it is more efficient to use the
vectorized statement as shown above.</p>
</div>
</div>
<div id="discrete-outcomes-with-gaussian-processes" class="section level3 unnumbered">
<h3>Discrete outcomes with Gaussian Processes</h3>
<p>Gaussian processes can be generalized the same way as standard linear
models by introducing a link function. This allows them to be used as
discrete data models.</p>
<div id="poisson-gp" class="section level4 unnumbered">
<h4>Poisson GP</h4>
<p>If we want to model count data, we can remove the <span class="math inline">\(\sigma\)</span> parameter, and use
<code>poisson_log</code>, which implements a log link, for our likelihood rather
than <code>normal</code>. We can also add an overall mean parameter, <span class="math inline">\(a\)</span>, which
will account for the marginal expected value for <span class="math inline">\(y\)</span>. We do this because we
cannot center count data like we would for normally distributed data.</p>
<pre><code>data {
...
  int&lt;lower=0&gt; y[N];
...
}
...
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real a;
  vector[N] eta;
}
model {
...
  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  a ~ std_normal();
  eta ~ std_normal();

  y ~ poisson_log(a + f);
}</code></pre>
</div>
<div id="logistic-gaussian-process-regression" class="section level4 unnumbered">
<h4>Logistic Gaussian Process Regression</h4>
<p>For binary classification problems, the observed outputs <span class="math inline">\(z_n \in \{ 0,1 \}\)</span> are binary. These outputs are modeled using a Gaussian
process with (unobserved) outputs <span class="math inline">\(y_n\)</span> through the logistic link,
<span class="math display">\[
z_n \sim \mathsf{Bernoulli}(\mbox{logit}^{-1}(y_n)),
\]</span>
or in other words,
<span class="math display">\[
\mbox{Pr}[z_n = 1] = \mbox{logit}^{-1}(y_n).
\]</span></p>
<p>We can extend our latent variable GP Stan program to deal with classification
problems. Below <span class="math inline">\(a\)</span> is the bias term, which can help account for imbalanced
classes in the training data:</p>
<pre><code>data {
...
  int&lt;lower=0, upper=1&gt; z[N];
...
}
...
model {
...

  y ~ bernoulli_logit(a + f);
}</code></pre>
</div>
</div>
<div id="automatic-relevance-determination" class="section level3 unnumbered">
<h3>Automatic Relevance Determination</h3>
<p>If we have multivariate inputs <span class="math inline">\(x \in \mathbb{R}^D\)</span>, the squared exponential
covariance function can be further generalized by fitting a scale
parameter <span class="math inline">\(\rho_d\)</span> for each dimension <span class="math inline">\(d\)</span>,
<span class="math display">\[
  k(x | \alpha, \vec{\rho}, \sigma)_{i, j} = \alpha^2 \exp
\left(-\dfrac{1}{2}
\sum_{d=1}^D \dfrac{1}{\rho_d^2} (x_{i,d} - x_{j,d})^2
\right)
+ \delta_{i, j}\sigma^2.
\]</span>
The estimation of <span class="math inline">\(\rho\)</span> was termed “automatic relevance determination” in
<span class="citation">Neal (<a href="#ref-Neal:1996">1996</a><a href="#ref-Neal:1996">a</a>)</span>, but this is misleading, because the magnitude the scale of
the posterior for each <span class="math inline">\(\rho_d\)</span> is dependent on the scaling of the input data
along dimension <span class="math inline">\(d\)</span>. Moreover, the scale of the parameters <span class="math inline">\(\rho_d\)</span> measures
non-linearity along the <span class="math inline">\(d\)</span>-th dimension, rather than “relevance”
<span class="citation">Piironen and Vehtari (<a href="#ref-PiironenVehtari:2016">2016</a>)</span>.</p>
<p>A priori, the closer <span class="math inline">\(\rho_d\)</span> is to zero, the more nonlinear the
conditional mean in dimension <span class="math inline">\(d\)</span> is. A posteriori, the actual dependencies
between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> play a role. With one covariate <span class="math inline">\(x_1\)</span> having a
linear effect and another covariate <span class="math inline">\(x_2\)</span> having a nonlinear effect,
it is possible that <span class="math inline">\(\rho_1 &gt; \rho_2\)</span> even if the predictive relevance
of <span class="math inline">\(x_1\)</span> is higher <span class="citation">(Rasmussen and Williams <a href="#ref-RasmussenWilliams:2006">2006</a>, 80)</span>.
The collection of <span class="math inline">\(\rho_d\)</span> (or <span class="math inline">\(1/\rho_d\)</span>) parameters can also be
modeled hierarchically.</p>
<p>The implementation of automatic relevance determination in Stan is
straightforward, though it currently requires the user to directly code the
covariance matrix. We’ll write a function to generate the Cholesky of the
covariance matrix called <code>L_cov_exp_quad_ARD</code>.</p>
<pre><code>functions {
  matrix L_cov_exp_quad_ARD(vector[] x,
                            real alpha,
                            vector rho,
                            real delta) {
    int N = size(x);
    matrix[N, N] K;
    real sq_alpha = square(alpha);
    for (i in 1:(N-1)) {
      K[i, i] = sq_alpha + delta;
      for (j in (i + 1):N) {
        K[i, j] = sq_alpha
                      * exp(-0.5 * dot_self((x[i] - x[j]) ./ rho));
        K[j, i] = K[i, j];
      }
    }
    K[N, N] = sq_alpha + delta;
    return cholesky_decompose(K);
  }
}
data {
  int&lt;lower=1&gt; N;
  int&lt;lower=1&gt; D;
  vector[D] x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  vector&lt;lower=0&gt;[D] rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K = L_cov_exp_quad_ARD(x, alpha, rho, delta);
    f = L_K * eta;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}</code></pre>
</div>
<div id="priors-gp.section" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Priors for Gaussian Process Parameters</h3>
<p>Formulating priors for GP hyperparameters requires the analyst to consider the
inherent statistical properties of a GP, the GP’s purpose in the model, and the
numerical issues that may arise in Stan when estimating a GP.</p>
<p>Perhaps most importantly, the parameters <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\alpha\)</span> are weakly
identified <span class="citation">Zhang (<a href="#ref-zhang-gp:2004">2004</a>)</span>. The ratio of the two
parameters is well-identified, but in practice we put independent priors on the
two hyperparameters because these two quantities are more interpretable than
their ratio.</p>
<div id="priors-for-length-scale" class="section level4 unnumbered">
<h4>Priors for length-scale</h4>
<p>GPs are a flexible class of priors and, as such, can represent a wide spectrum
of functions. For length scales below the minimum spacing of the covariates
the GP likelihood plateaus. Unless regularized by a prior, this flat
likelihood induces considerable posterior mass at small length scales where the
observation variance drops to zero and the functions supported by the GP being
to exactly interpolate between the input data. The resulting posterior not
only significantly overfits to the input data, it also becomes hard to
accurately sample using Euclidean HMC.</p>
<p>We may wish to put further soft constraints on the length-scale, but these are
dependent on how the GP is used in our statistical model.</p>
<p>If our model consists of only the GP, i.e.:</p>
<p><span class="math display">\[ \begin{aligned}
  f &amp; \sim \mathsf{multivariate\ normal}\left(0, K(x | \alpha, \rho)\right) \\
  y_i &amp; \sim \mathsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\} \\
  &amp; x \in \mathbb{R}^{N \times D}, \, f \in \mathbb{R}^N
\end{aligned} \]</span></p>
<p>we likely don’t need constraints beyond penalizing small
length-scales. We’d like to allow the GP prior to represent both
high-frequency and low-frequency functions, so our prior should put
non-negligible mass on both sets of functions. In this case, an
inverse gamma, <code>inv_gamma_lpdf</code> in Stan’s language, will work
well as it has a sharp left tail that puts negligible mass on
infinitesimal length-scales, but a generous right tail, allowing for
large length-scales. Inverse gamma priors will avoid infinitesimal length-scales
because the density is zero at zero, so the posterior for length-scale will be
pushed away from zero. An inverse gamma distribution is one of many
zero-avoiding or boundary-avoiding distributions. See
 for more on boundary-avoiding priors.</p>
<p>If we’re using the GP as a component in a larger model that includes an overall
mean and fixed effects for the same variables we’re using as the domain for the
GP, i.e.:</p>
<p><span class="math display">\[ \begin{aligned}
  f &amp; \sim \mathsf{multivariate\ normal}\left(0, K(x | \alpha, \rho)\right) \\ y_i &amp;
  \sim \mathsf{normal}(\beta_0 + x_i \beta_{[1:D]} + f_i, \sigma) \, \forall i
  \in \{1, \dots, N\} \\ &amp; x_i^T, \beta_{[1:D]} \in \mathbb{R}^D,\, x \in \mathbb{R}^{N
  \times D},\, f \in \mathbb{R}^N
\end{aligned} \]</span></p>
<p>we’ll likely want to constrain large length-scales as well. A length scale
that is larger than the scale of the data yields a GP posterior that is
practically linear (with respect to the particular covariate) and increasing
the length scale has little impact on the likelihood. This will introduce
nonidentifiability in our model, as both the fixed effects and the GP will
explain similar variation. In order to limit the amount of overlap between the
GP and the linear regression, we should use a prior with a sharper right tail
to limit the GP to higher-frequency functions. We can use a generalized inverse
Gaussian distribution:</p>
<p><span class="math display">\[ \begin{aligned}
  f(x | a, b, p) &amp; = \dfrac{(a/b)^{p/2}}{2K_p(\sqrt{ab})} x^{p - 1}\exp(-(ax + b
  / x)/2) \\
  &amp; x, a, b \in \mathbb{R}^{+}, \, p \in \mathbb{Z}
\end{aligned} \]</span></p>
<p>which has an inverse gamma left tail if <span class="math inline">\(p \leq 0\)</span> and an inverse Gaussian
right tail. This has not yet been implemented in Stan’s math library, but it
is possible to implement as a user defined function:</p>
<pre><code>functions {
  real generalized_inverse_gaussian_lpdf(real x, int p,
                                        real a, real b) {
    return p * 0.5 * log(a / b)
      - log(2 * modified_bessel_second_kind(p, sqrt(a * b)))
      + (p - 1) * log(x)
      - (a * x + b / x) * 0.5;
 }
}
data {
...</code></pre>
<p>If we have high-frequency covariates in our fixed effects, we may wish to
further regularize the GP away from high-frequency functions, which means we’ll
need to penalize smaller length-scales. Luckily, we have a useful way of
thinking about how length-scale affects the frequency of the functions
supported the GP. If we were to repeatedly draw from a zero-mean GP with a
length-scale of <span class="math inline">\(\rho\)</span> in a fixed-domain <span class="math inline">\([0,T]\)</span>, we would get a distribution
for the number of times each draw of the GP crossed the zero axis. The
expectation of this random variable, the number of zero crossings, is <span class="math inline">\(T / \pi \rho\)</span>. You can see that as <span class="math inline">\(\rho\)</span> decreases, the expectation of the number of
upcrossings increases as the GP is representing higher-frequency functions.
Thus, this is a good statistic to keep in mind when setting a lower-bound for
our prior on length-scale in the presence of high-frequency covariates.
However, this statistic is only valid for one-dimensional inputs.</p>
</div>
<div id="priors-for-marginal-standard-deviation" class="section level4 unnumbered">
<h4>Priors for marginal standard deviation</h4>
<p>The parameter <span class="math inline">\(\alpha\)</span> corresponds to how much of the variation is
explained by the regression function and has a similar role to the
prior variance for linear model weights. This means the prior can be
the same as used in linear models, such as a half-<span class="math inline">\(t\)</span> prior on <span class="math inline">\(\alpha\)</span>.</p>
<p>A half-<span class="math inline">\(t\)</span> or half-Gaussian prior on alpha also has the benefit of putting
nontrivial prior mass around zero. This allows the GP support the zero
functions and allows the possibility that the GP won’t contribute to the
conditional mean of the total output.</p>
</div>
</div>
<div id="predictive-inference-with-a-gaussian-process" class="section level3 unnumbered">
<h3>Predictive Inference with a Gaussian Process</h3>
<p>Suppose for a given sequence of inputs <span class="math inline">\(x\)</span> that the corresponding
outputs <span class="math inline">\(y\)</span> are observed. Given a new sequence of inputs <span class="math inline">\(\tilde{x}\)</span>,
the posterior predictive distribution of their labels is computed by
sampling outputs <span class="math inline">\(\tilde{y}\)</span> according to
<span class="math display">\[
p(\tilde{y}|\tilde{x},x,y)
\ = \
\frac{p(\tilde{y}, y|\tilde{x},x)}
     {p(y|x)}
\ \propto \
p(\tilde{y}, y|\tilde{x},x).
\]</span></p>
<p>A direct implementation in Stan defines a model in terms of the
joint distribution of the observed <span class="math inline">\(y\)</span> and unobserved <span class="math inline">\(\tilde{y}\)</span>.</p>
<pre><code>data {
  int&lt;lower=1&gt; N1;
  real x1[N1];
  vector[N1] y1;
  int&lt;lower=1&gt; N2;
  real x2[N2];
}
transformed data {
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  real x[N];
  for (n1 in 1:N1) x[n1] = x1[n1];
  for (n2 in 1:N2) x[N1 + n2] = x2[n2];
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y1 ~ normal(f[1:N1], sigma);
}
generated quantities {
  vector[N2] y2;
  for (n2 in 1:N2)
    y2[n2] = normal_rng(f[N1 + n2], sigma);
}</code></pre>
<p>The input vectors <code>x1</code> and <code>x2</code> are declared as data, as is the
observed output vector <code>y1</code>. The unknown output vector <code>y2</code>, which
corresponds to input vector <code>x2</code>, is declared in the generated quantities
block and will be sampled when the model is executed.</p>
<p>A transformed data block is used to combine the input vectors
<code>x1</code> and <code>x2</code> into a single vector <code>x</code>.</p>
<p>The model block declares and defines a local variable for the combined output
vector <code>f</code>, which consists of the concatenation of the conditional mean
for known outputs <code>y1</code> and unknown outputs <code>y2</code>. Thus the
combined output vector <code>f</code> is aligned with the combined
input vector <code>x</code>. All that is left is to define the univariate
normal sampling statement for <code>y</code>.</p>
<p>The generated quantities block defines the quantity <code>y2</code>. We generate
<code>y2</code> by sampling <code>N2</code> univariate normals with each mean corresponding
to the appropriate element in <code>f</code>.</p>
<div id="predictive-inference-in-non-gaussian-gps" class="section level4 unnumbered">
<h4>Predictive Inference in non-Gaussian GPs</h4>
<p>We can do predictive inference in non-Gaussian GPs in much the
same way as we do with Gaussian GPs.</p>
<p>Consider the following full model for prediction using logistic Gaussian
process regression.</p>
<pre><code>data {
  int&lt;lower=1&gt; N1;
  real x1[N1];
  int&lt;lower=0, upper=1&gt; z1[N1];
  int&lt;lower=1&gt; N2;
  real x2[N2];
}
transformed data {
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  real x[N];
  for (n1 in 1:N1) x[n1] = x1[n1];
  for (n2 in 1:N2) x[N1 + n2] = x2[n2];
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real a;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  a ~ std_normal();
  eta ~ std_normal();

  z1 ~ bernoulli_logit(a + f[1:N1]);
}
generated quantities {
  int z2[N2];
  for (n2 in 1:N2)
    z2[n2] = bernoulli_logit_rng(a + f[N1 + n2]);
}</code></pre>
</div>
<div id="analytical-form-of-joint-predictive-inference" class="section level4 unnumbered">
<h4>Analytical Form of Joint Predictive Inference</h4>
<p>Bayesian predictive inference for Gaussian processes with Gaussian observations
can be sped up by deriving the posterior analytically, then directly sampling
from it.</p>
<p>Jumping straight to the result,
<span class="math display">\[
p(\tilde{y}|\tilde{x},y,x)
=
\mathsf{normal}(K^{\top}\Sigma^{-1}y,\
                \Omega - K^{\top}\Sigma^{-1}K),
\]</span>
where <span class="math inline">\(\Sigma = K(x | \alpha, \rho, \sigma)\)</span> is the result of applying the covariance
function to the inputs <span class="math inline">\(x\)</span> with observed outputs <span class="math inline">\(y\)</span>, <span class="math inline">\(\Omega = K(\tilde{x} | \alpha, \rho)\)</span> is the result of applying the covariance function to the
inputs <span class="math inline">\(\tilde{x}\)</span> for which predictions are to be inferred, and <span class="math inline">\(K\)</span>
is the matrix of covariances between inputs <span class="math inline">\(x\)</span> and <span class="math inline">\(\tilde{x}\)</span>, which
in the case of the exponentiated quadratic covariance function
would be</p>
<p><span class="math display">\[
K(x | \alpha, \rho)_{i, j} = \eta^2 \exp(-\dfrac{1}{2 \rho^2}
\sum_{d=1}^D (x_{i,d} - \tilde{x}_{j,d})^2).
\]</span></p>
<p>There is no noise term including <span class="math inline">\(\sigma^2\)</span> because the indexes of
elements in <span class="math inline">\(x\)</span> and <span class="math inline">\(\tilde{x}\)</span> are never the same.</p>
<p>This Stan code below uses the analytic form of the posterior and provides
sampling of the resulting multivariate normal through the Cholesky
decomposition. The data declaration is the same as for the latent variable
example, but we’ve defined a function called <code>gp_pred_rng</code> which will
generate a draw from the posterior predictive mean conditioned on observed data
<code>y1</code>. The code uses a Cholesky decomposition in triangular solves in order
to cut down on the the number of matrix-matrix multiplications when computing
the conditional mean and the conditional covariance of <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<pre><code>functions {
  vector gp_pred_rng(real[] x2,
                     vector y1,
                     real[] x1,
                     real alpha,
                     real rho,
                     real sigma,
                     real delta) {
    int N1 = rows(y1);
    int N2 = size(x2);
    vector[N2] f2;
    {
      matrix[N1, N1] L_K;
      vector[N1] K_div_y1;
      matrix[N1, N2] k_x1_x2;
      matrix[N1, N2] v_pred;
      vector[N2] f2_mu;
      matrix[N2, N2] cov_f2;
      matrix[N2, N2] diag_delta;
      matrix[N1, N1] K;
      K = cov_exp_quad(x1, alpha, rho);
      for (n in 1:N1)
        K[n, n] = K[n,n] + square(sigma);
      L_K = cholesky_decompose(K);
      K_div_y1 = mdivide_left_tri_low(L_K, y1);
      K_div_y1 = mdivide_right_tri_low(K_div_y1&#39;, L_K)&#39;;
      k_x1_x2 = cov_exp_quad(x1, x2, alpha, rho);
      f2_mu = (k_x1_x2&#39; * K_div_y1);
      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);
      cov_f2 = cov_exp_quad(x2, alpha, rho) - v_pred&#39; * v_pred;
      diag_delta = diag_matrix(rep_vector(delta, N2));

      f2 = multi_normal_rng(f2_mu, cov_f2 + diag_delta);
    }
    return f2;
  }
}
data {
  int&lt;lower=1&gt; N1;
  real x1[N1];
  vector[N1] y1;
  int&lt;lower=1&gt; N2;
  real x2[N2];
}
transformed data {
  vector[N1] mu = rep_vector(0, N1);
  real delta = 1e-9;
}
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
}
model {
  matrix[N1, N1] L_K;
  {
    matrix[N1, N1] K = cov_exp_quad(x1, alpha, rho);
    real sq_sigma = square(sigma);

    // diagonal elements
    for (n1 in 1:N1)
      K[n1, n1] = K[n1, n1] + sq_sigma;

    L_K = cholesky_decompose(K);
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();

  y1 ~ multi_normal_cholesky(mu, L_K);
}
generated quantities {
  vector[N2] f2;
  vector[N2] y2;

  f2 = gp_pred_rng(x2, y1, x1, alpha, rho, sigma, delta);
  for (n2 in 1:N2)
    y2[n2] = normal_rng(f2[n2], sigma);
}</code></pre>
</div>
</div>
<div id="multiple-output-gaussian-processes" class="section level3 unnumbered">
<h3>Multiple-output Gaussian processes</h3>
<p>Suppose we have observations <span class="math inline">\(y_i \in \mathbb{R}^M\)</span> observed at
<span class="math inline">\(x_i \in \mathbb{R}^K\)</span>. One can model the data like so:
<span class="math display">\[ \begin{aligned}
  y_i &amp; \sim \mathsf{multivariate\ normal}(f(x_i), \mathbf{I}_M \sigma^2) \\
  f(x) &amp; \sim \mathsf{GP}(m(x), K(x | \theta, \phi)) \\
  K(x &amp; | \theta) \in \mathbb{R}^{M \times M}, \, f(x), \, m(x) \in \mathbb{R}^M
\end{aligned} \]</span>
where the <span class="math inline">\(K(x, x^\prime | \theta, \phi)_{[m, m^\prime]}\)</span> entry defines the
covariance between <span class="math inline">\(f_m(x)\)</span> and <span class="math inline">\(f_{m^\prime}(x^\prime)(x)\)</span>. This construction
of Gaussian processes allows us to learn the covariance between the output
dimensions of <span class="math inline">\(f(x)\)</span>. If we parameterize our kernel <span class="math inline">\(K\)</span>:
<span class="math display">\[ \begin{aligned} K(x, x^\prime | \theta, \phi)_{[m, m^\prime]} = k(x, x^\prime |
\theta) k(m, m^\prime | \phi) \end{aligned} \]</span>
then our finite dimensional generative model for the above is:
<span class="math display">\[ \begin{aligned}
  f &amp; \sim \mathsf{Matrixnormalal}(m(x), K(x | \alpha, \rho), C(\phi)) \\
  y_{i, m} &amp; \sim \mathsf{normal}(f_{i,m}, \sigma) \\
  f &amp; \in \mathbb{R}^{N \times M}
\end{aligned} \]</span>
where <span class="math inline">\(K(x | \alpha, \rho)\)</span> is the exponentiated quadratic kernel we’ve used
throughout this chapter, and <span class="math inline">\(C(\phi)\)</span> is a positive-definite matrix,
parameterized by some vector <span class="math inline">\(\phi\)</span>.</p>
<p>The matrix normal distribution has two covariance matrices: <span class="math inline">\(K(x | \alpha, \rho)\)</span> to encode column covariance, and <span class="math inline">\(C(\phi)\)</span> to define row
covariance. The salient features of the matrix normal are that the rows
of the matrix <span class="math inline">\(f\)</span> are distributed:
<span class="math display">\[ \begin{aligned} f_{[n,]} \sim \mathsf{multivariate\ normal}(m(x)_{[n,]}, K(x | \alpha,
\rho)_{[n,n]} C(\phi)) \end{aligned} \]</span> and that the columns of the matrix <span class="math inline">\(f\)</span> are
distributed: <span class="math display">\[ \begin{aligned} f_{[,m]} \sim \mathsf{multivariate\ normal}(m(x)_{[,m]}, K(x
  | \alpha, \rho) C(\phi)_{[m,m]}) \end{aligned} \]</span>
This also means means that <span class="math inline">\(\mathbb{E}\left[f^T f\right]\)</span> is equal to
<span class="math inline">\(\text{trace}(K(x | \alpha, \rho)) * C\)</span>, whereas <span class="math inline">\(\mathbb{E}\left[ff^T\right]\)</span>
is <span class="math inline">\(\text{trace}(C) * K(x | \alpha, \rho)\)</span>. We can derive this using
properties of expectation and the matrix normal density.</p>
<p>We should set <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(1.0\)</span> because the parameter is not identified unless
we constrain <span class="math inline">\(\text{trace}(C) = 1\)</span>. Otherwise, we can multiply <span class="math inline">\(\alpha\)</span> by a scalar <span class="math inline">\(d\)</span> and
<span class="math inline">\(C\)</span> by <span class="math inline">\(1/d\)</span> and our likelihood will not change.</p>
<p>We can generate a random variable <span class="math inline">\(f\)</span> from a matrix normal density in
<span class="math inline">\(\mathbb{R}^{N \times M}\)</span> using the following algorithm:</p>
<p><span class="math display">\[ \begin{aligned}
  \eta_{i,j} &amp; \sim \mathsf{normal}(0, 1) \, \forall i,j \\
  f &amp; = L_{K(x | 1.0, \rho)} \, \eta \, L_C(\phi)^T \\
  f &amp; \sim \mathsf{MatrixNormal}(0, K(x | 1.0, \rho), C(\phi)) \\
  \eta &amp; \in \mathbb{R}^{N \times M} \\
  L_C(\phi) &amp; = \text{cholesky\_decompose}(C(\phi)) \\
  L_{K(x | 1.0, \rho)} &amp; = \text{cholesky\_decompose}(K(x | 1.0, \rho))
\end{aligned} \]</span></p>
<p>This can be implemented in Stan using a latent-variable GP formulation. We’ve used
<span class="math inline">\(\mathsf{LkjCorr}\)</span> for <span class="math inline">\(C(\phi)\)</span>, but any positive-definite matrix will do.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  int&lt;lower=1&gt; D;
  real x[N];
  matrix[N, D] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  real&lt;lower=0&gt; rho;
  vector&lt;lower=0&gt;[D] alpha;
  real&lt;lower=0&gt; sigma;
  cholesky_factor_corr[D] L_Omega;
  matrix[N, D] eta;
}
model {
  matrix[N, D] f;
  {
    matrix[N, N] K = cov_exp_quad(x, 1.0, rho);
    matrix[N, N] L_K;

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta
        * diag_pre_multiply(alpha, L_Omega)&#39;;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(3);
  to_vector(eta) ~ std_normal();

  to_vector(y) ~ normal(to_vector(f), sigma);
}
generated quantities {
  matrix[D, D] Omega;
  Omega = L_Omega * L_Omega&#39;;
}</code></pre>

</div>
</div>
<!-- </div> -->
<h3><i style="font-size: 110%; color:#990017;">References</i></h3>
<div id="refs" class="references">
<div id="ref-Neal:1997">
<p>Neal, Radford M. 1997. “Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification.” 9702. University of Toronto, Department of Statistics.</p>
</div>
<div id="ref-Neal:1996">
<p>Neal, Radford M. 1996a. <em>Bayesian Learning for Neural Networks</em>. Lecture Notes in Statistics 118. New York: Springer.</p>
</div>
<div id="ref-PiironenVehtari:2016">
<p>Piironen, Juho, and Aki Vehtari. 2016. “Projection Predictive Model Selection for Gaussian Processes.” In <em>Machine Learning for Signal Processing (Mlsp), 2016 Ieee 26th International Workshop on</em>. IEEE.</p>
</div>
<div id="ref-RasmussenWilliams:2006">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. MIT Press.</p>
</div>
<div id="ref-zhang-gp:2004">
<p>Zhang, Hao. 2004. “Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics.” <em>Journal of the American Statistical Association</em> 99 (465). Taylor &amp; Francis: 250–61.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simulating-from-a-gaussian-process.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="directions-rotations-and-hyperspheres.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
