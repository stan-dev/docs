<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Statistics Using Stan</title>
  <meta name="description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Statistics Using Stan" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="img/logo-tm.pdf" />
  <meta property="og:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics Using Stan" />
  
  <meta name="twitter:description" content="Bayesian Statistics Using Stan, including Stan user’s guide with examples and programming techniques." />
  <meta name="twitter:image" content="img/logo-tm.pdf" />

<meta name="author" content="Stan Development Team">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="simple-examples.html">
<link rel="next" href="example-models-part.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="stan-manual.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style="font-size:110%; font-weight:400; font-family: Verdana, Helvetica, sans; line-height:1.4; margin: 0.5em 0 0 1em">Bayesian Statistics with Stan</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Book</a></li>
<li><a href="part-1-overview.html#part-1-overview"><i style="font-size: 110%; padding:1.5em 0 0 0; color:#990017;">Part 1: Overview</i></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prior-distributions-and-models-for-data.html"><a href="prior-distributions-and-models-for-data.html"><i class="fa fa-check"></i><b>2</b> Prior Distributions and Models for Data</a></li>
<li class="chapter" data-level="3" data-path="simple-examples.html"><a href="simple-examples.html"><i class="fa fa-check"></i><b>3</b> Simple Examples</a></li>
<li class="chapter" data-level="4" data-path="bayesian-workflow-1.html"><a href="bayesian-workflow-1.html"><i class="fa fa-check"></i><b>4</b> Bayesian Workflow</a></li>
<li><a href="example-models-part.html#example-models.part"><i style="font-size: 110%; color:#990017;">Part 2. Example Models</i></span></a></li>
<li class="chapter" data-level="5" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>5</b> Regression Models</a></li>
<li class="chapter" data-level="6" data-path="time-series-chapter.html"><a href="time-series-chapter.html"><i class="fa fa-check"></i><b>6</b> Time-Series Models</a></li>
<li class="chapter" data-level="7" data-path="missing-data-and-partially-known-parameters.html"><a href="missing-data-and-partially-known-parameters.html"><i class="fa fa-check"></i><b>7</b> Missing Data and Partially Known Parameters</a></li>
<li class="chapter" data-level="8" data-path="floating-point-arithmetic.html"><a href="floating-point-arithmetic.html"><i class="fa fa-check"></i><b>8</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="9" data-path="truncated-or-censored-data.html"><a href="truncated-or-censored-data.html"><i class="fa fa-check"></i><b>9</b> Truncated or Censored Data</a></li>
<li class="chapter" data-level="10" data-path="mixture-modeling-chapter.html"><a href="mixture-modeling-chapter.html"><i class="fa fa-check"></i><b>10</b> Finite Mixtures</a></li>
<li class="chapter" data-level="11" data-path="measurement-error-and-meta-analysis.html"><a href="measurement-error-and-meta-analysis.html"><i class="fa fa-check"></i><b>11</b> Measurement Error and Meta-Analysis</a></li>
<li class="chapter" data-level="12" data-path="latent-discrete-chapter.html"><a href="latent-discrete-chapter.html"><i class="fa fa-check"></i><b>12</b> Latent Discrete Parameters</a></li>
<li class="chapter" data-level="13" data-path="sparse-ragged-chapter.html"><a href="sparse-ragged-chapter.html"><i class="fa fa-check"></i><b>13</b> Sparse and Ragged Data Structures</a></li>
<li class="chapter" data-level="14" data-path="clustering-chapter.html"><a href="clustering-chapter.html"><i class="fa fa-check"></i><b>14</b> Clustering Models</a></li>
<li class="chapter" data-level="15" data-path="gaussian-processes-chapter.html"><a href="gaussian-processes-chapter.html"><i class="fa fa-check"></i><b>15</b> Gaussian Processes</a></li>
<li class="chapter" data-level="16" data-path="directions-rotations-and-hyperspheres.html"><a href="directions-rotations-and-hyperspheres.html"><i class="fa fa-check"></i><b>16</b> Directions, Rotations, and Hyperspheres</a></li>
<li class="chapter" data-level="17" data-path="algebra-solver-chapter.html"><a href="algebra-solver-chapter.html"><i class="fa fa-check"></i><b>17</b> Solving Algebraic Equations</a></li>
<li class="chapter" data-level="18" data-path="ode-solver-chapter.html"><a href="ode-solver-chapter.html"><i class="fa fa-check"></i><b>18</b> Ordinary Differential Equations</a></li>
<li><a href="part-3-programming-techniques.html#part-3.-programming-techniques"><i style="font-size: 110%; color:#990017;">Part 3. Programming Techniques</i></a></li>
<li class="chapter" data-level="19" data-path="modeling-as-software-development.html"><a href="modeling-as-software-development.html"><i class="fa fa-check"></i><b>19</b> Modeling as Software Development</a></li>
<li class="chapter" data-level="20" data-path="matrices-vectors-and-arrays.html"><a href="matrices-vectors-and-arrays.html"><i class="fa fa-check"></i><b>20</b> Matrices, Vectors, and Arrays</a></li>
<li class="chapter" data-level="21" data-path="multi-indexing-chapter.html"><a href="multi-indexing-chapter.html"><i class="fa fa-check"></i><b>21</b> Multiple Indexing and Range Indexing</a></li>
<li class="chapter" data-level="22" data-path="functions-programming-chapter.html"><a href="functions-programming-chapter.html"><i class="fa fa-check"></i><b>22</b> User-Defined Functions</a></li>
<li class="chapter" data-level="23" data-path="custom-probability-functions-chapter.html"><a href="custom-probability-functions-chapter.html"><i class="fa fa-check"></i><b>23</b> Custom Probability Functions</a></li>
<li class="chapter" data-level="24" data-path="problematic-posteriors-chapter.html"><a href="problematic-posteriors-chapter.html"><i class="fa fa-check"></i><b>24</b> Problematic Posteriors</a></li>
<li class="chapter" data-level="25" data-path="change-of-variables-chapter.html"><a href="change-of-variables-chapter.html"><i class="fa fa-check"></i><b>25</b> Reparameterization and Change of Variables</a></li>
<li class="chapter" data-level="26" data-path="optimization-chapter.html"><a href="optimization-chapter.html"><i class="fa fa-check"></i><b>26</b> Efficiency Tuning</a></li>
<li class="chapter" data-level="27" data-path="map-reduce-chapter.html"><a href="map-reduce-chapter.html"><i class="fa fa-check"></i><b>27</b> Map-Reduce</a></li>
<li><a href="part-4-review-of-statistical-inference.html#part-4-review-of-statistical-inference"><i style="font-size: 110%; color:#990017;">Part 4: Review of Statistical Inference</i></a></li>
<li class="chapter" data-level="28" data-path="bayesian-data-analysis-1.html"><a href="bayesian-data-analysis-1.html"><i class="fa fa-check"></i><b>28</b> Bayesian Data Analysis</a></li>
<li class="chapter" data-level="29" data-path="mle-chapter.html"><a href="mle-chapter.html"><i class="fa fa-check"></i><b>29</b> Penalized Maximum Likelihood Point Estimation</a></li>
<li class="chapter" data-level="30" data-path="bayesian-point-estimation.html"><a href="bayesian-point-estimation.html"><i class="fa fa-check"></i><b>30</b> Bayesian Point Estimation</a></li>
<li class="chapter" data-level="31" data-path="vi-advanced-chapter.html"><a href="vi-advanced-chapter.html"><i class="fa fa-check"></i><b>31</b> Variational Inference</a></li>
<li><a href="appendices.html#appendices"><i style="font-size: 110%; color:#990017;">Appendices</i></a></li>
<li class="chapter" data-level="" data-path="appendix-1-stan-program-style-guide.html"><a href="appendix-1-stan-program-style-guide.html"><i class="fa fa-check"></i>Appendix 1. Stan Program Style Guide</a></li>
<li class="chapter" data-level="" data-path="stan-for-bugs-appendix.html"><a href="stan-for-bugs-appendix.html"><i class="fa fa-check"></i>Appendix 2. Transitioning from BUGS</a></li>
<li><a href="references.html#references"><i style="font-size: 110%; color:#990017;">References</i></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics Using Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-workflow-1" class="section level1">
<h1><span class="header-section-number">4</span> Bayesian Workflow</h1>
<p>In this chapter we go through an artificial probem from beginning to
end, starting with the (hypothetical) scenario and the (simulated)
data and then fitting a series of Bayesian models in Stan and
exploring them in R.</p>
<p>The other chapters of this book are focused on building Stan models.
This chapter is a bit different in focusing on the Bayesian data
analysis workflow of model building, checking, and expansion within R.</p>
<div id="the-problem" class="section level2">
<h2><span class="header-section-number">4.1</span> The problem</h2>
<div id="background" class="section level3 unnumbered">
<h3>Background</h3>
<p>Imagine that you are a statistician or data scientist working as an
independent contractor. One of your clients is a company that owns
many residential buildings throughout New York City. The property
manager explains that they are concerned about the number of cockroach
complaints that they receive from their buildings. Previously the
company has offered monthly visits from a pest inspector as a solution
to this problem. While this is the default solution of many property
managers in NYC, the tenants are rarely home when the inspector
visits, and so the manager reasons that this is a relatively expensive
solution that is currently not very effective.</p>
<p>One alternative is to deploy long term bait stations. In this
alternative, child and pet safe bait stations are installed throughout
the apartment building. Cockroaches obtain quick acting poison from
these stations and distribute it throughout the colony. The
manufacturer of these bait stations provides some indication of the
space-to-bait efficacy, but the manager suspects that this guidance
was not calculated with NYC roaches in mind. NYC roaches, the manager
rationalizes, have more hustle than traditional roaches; and NYC
buildings are built differently than other common residential
buildings in the US. This is particularly important as the unit cost
for each bait station per year is high.</p>
</div>
<div id="the-goal" class="section level3 unnumbered">
<h3>The goal</h3>
<p>The manager wishes to employ your services to help them to find the
optimal number of roach bait stations they should place in each of
their buildings in order to minimize the number of cockroach
complaints while also keeping expenditure on pest control affordable.</p>
<p>A subset of the company’s buildings have been randomly selected for an
experiment:</p>
<ul>
<li><p>At the beginning of each month, a pest inspector randomly places a
number of bait stations throughout the building, without knowledge of
the current cockroach levels in the building.</p></li>
<li><p>At the end of the month, the manager records the total number of
cockroach complaints in that building.</p></li>
<li><p>The manager would like to determine the optimal number of bait
stations (<span class="math inline">\(\textrm{traps}\)</span>) that balances the lost revenue
(<span class="math inline">\(\textrm{R}\)</span>) that complaints generate with the all-in cost of
maintaining the bait stations (<span class="math inline">\(\textrm{TC}\)</span>).</p></li>
</ul>
<p>Fortunately, Bayesian data analysis provides a coherent framework for
us to tackle this problem.</p>
<p>Formally, we are interested in finding the number of bait stations
that maximixes
<span class="math display">\[
\mbox{E}(R(\textrm{complaints}(\textrm{traps})) - \textrm{TC}(\textrm{traps})),
\]</span>
where the expectation averages over the distribution of complaints,
conditional on the number of bait stations installed.</p>
<p>The property manager would also, if possible, like to learn how these
results generalize to buildings they haven’t treated so they can
understand the potential costs of pest control at buildings they are
acquiring as well as for the rest of their building portfolio.</p>
<p>As the property manager has complete control over the number of bait
stations set, the random variable contributing to this expectation is
the number of complaints given the number of bait stations. We will
model the number of complaints as a function of the number of bait
stations.</p>
</div>
</div>
<div id="the-data" class="section level2">
<h2><span class="header-section-number">4.2</span> The data</h2>
<p>The (simulated) data for this problem are in a file called
<code>pest_data.RDS</code>, representing data from 10 buildings in 12 successive
months, thus 120 data points in total. Let’s load the data and see
what the structure is:</p>
<pre><code>&#39;data.frame&#39;:   120 obs. of  14 variables:
 $ mus                 : num  0.369 0.359 0.282 0.129 0.452 ...
 $ building_id         : int  37 37 37 37 37 37 37 37 37 37 ...
 $ wk_ind              : int  1 2 3 4 5 6 7 8 9 10 ...
 $ date                : Date, format: &quot;2017-01-15&quot; &quot;2017-02-14&quot; ...
 $ traps               : num  8 8 9 10 11 11 10 10 9 9 ...
 $ floors              : num  8 8 8 8 8 8 8 8 8 8 ...
 $ sq_footage_p_floor  : num  5149 5149 5149 5149 5149 ...
 $ live_in_super       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ monthly_average_rent: num  3847 3847 3847 3847 3847 ...
 $ average_tenant_age  : num  53.9 53.9 53.9 53.9 53.9 ...
 $ age_of_building     : num  47 47 47 47 47 47 47 47 47 47 ...
 $ total_sq_foot       : num  41192 41192 41192 41192 41192 ...
 $ month               : num  1 2 3 4 5 6 7 8 9 10 ...
 $ complaints          : num  1 3 0 1 0 0 4 3 2 2 ...</code></pre>
<p>These are the variables we will be using:</p>
<ul>
<li><code>building_id</code>: The unique building identifier</li>
<li><code>traps</code>: The number of traps used in the building in that month</li>
<li><code>floors</code>: The number of floors in the building</li>
<li><code>live_in_super</code>: An indicator for whether the building has a live-in
superintendent</li>
<li><code>monthly_average_rent</code>: The average monthly rent in the building</li>
<li><code>average_tenant_age</code>: The aerage age of the tenants in the building</li>
<li><code>age_of_building</code>: The age of the building</li>
<li><code>total_sq_foot</code>: The total square footage in the building</li>
<li><code>month</code>: Month of the year</li>
<li><code>complaints</code>: Number of complaints in the building in that month</li>
</ul>
<p>First, let’s see how many buildings we have data for:</p>
<pre><code>[1] 10</code></pre>
<p>And make some plots: a histogram of the number of complaints in the
120 building-months in the data:</p>
<p><img src="workflow_files/figure-html/data-plots-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>A scatterplot of complaints vs. bait stations, with each dot
representing a building-month:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>And the time series of bait stations and complaints for each building:</p>
<p><img src="workflow_files/figure-html/data-plots-ts-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We will first just look at the association of the number of bait
stations with the number of complaints, ignoring systamtic variation
over time and across buildings (we’ll come back to those sources of
variation later in the chapter). That requires only two variables,
<span class="math inline">\(\textrm{complaints}\)</span> and <span class="math inline">\(\textrm{traps}\)</span>.</p>
<p>How should we model the number of complaints? We will demonstrate
using a Bayesian workflow of model building, model checking, and model
improvement.</p>
</div>
<div id="modeling-count-data-poisson-distribution" class="section level2">
<h2><span class="header-section-number">4.3</span> Modeling count data: Poisson distribution</h2>
<p>We already know some rudimentary information about what we should
expect. The number of complaints over a month should be either zero or
a positive integer. The property manager tells us that it is possible
but unlikely that number of complaints in a given month is
zero. Occasionally there are a large number of complaints in a single
month. A common way of modeling this sort of skewed, single bounded
count data is as a Poisson random variable. One concern about modeling
the outcome variable as Poisson is that the data may be
over-dispersed, but we’ll start with the Poisson model and then check
whether over-dispersion is a problem by comparing our model’s
predictions to the data.</p>
<div id="model" class="section level3 unnumbered">
<h3>Model</h3>
<p>Given that we have chosen a Poisson regression, we define the
likelihood to be the Poisson probability mass function over the number
of bait stations placed in the building, denoted below as
<code>traps</code>. This model assumes that the mean and variance of the outcome
variable <code>complaints</code> (number of complaints) is the same. We’ll
investigate whether this is a good assumption after we fit the model.</p>
<p>For building <span class="math inline">\(b = 1,\dots,10\)</span> at time (month) <span class="math inline">\(t = 1,\dots,12\)</span>, we
have</p>
<p><span class="math display">\[ \begin{aligned}
\textrm{complaints}_{b,t} &amp; \sim \textrm{Poisson}(\lambda_{b,t}) \\
\lambda_{b,t} &amp; = \exp{(\eta_{b,t})} \\
\eta_{b,t} &amp;= \alpha + \beta \, \textrm{traps}_{b,t}
\end{aligned} \]</span></p>
<p>Let’s encode this probability model in a Stan program.</p>
</div>
<div id="writing-our-first-stan-model" class="section level3 unnumbered">
<h3>Writing our first Stan model</h3>
<pre><code>functions {
  /*
  * Alternative to poisson_log_rng() that 
  * avoids potential numerical problems during warmup
  */
  int poisson_log_safe_rng(real eta) {
    real pois_rate = exp(eta);
    if (pois_rate &gt;= exp(20.79))
      return -9;
    return poisson_rng(pois_rate);
  }
}
data {
  int&lt;lower=1&gt; N;
  int&lt;lower=0&gt; complaints[N];
  vector&lt;lower=0&gt;[N] traps;
}
parameters {
  real alpha;
  real beta;
}
model {
  // weakly informative priors:
  // we expect negative slope on traps and a positive intercept,
  // but we will allow ourselves to be wrong
  beta ~ normal(-0.25, 1);
  alpha ~ normal(log(4), 1);
  
  // poisson_log(eta) is more efficient and stable alternative to poisson(exp(eta))
  complaints ~ poisson_log(alpha + beta * traps);
} 
generated quantities {
  int y_rep[N]; 
  for (n in 1:N) 
    y_rep[n] = poisson_log_safe_rng(alpha + beta * traps[n]);
}</code></pre>
</div>
<div id="making-sure-our-code-is-right" class="section level3 unnumbered">
<h3>Making sure our code is right</h3>
<p>Before we fit the model to the data that have been given to us, we
should check that our model works well with data that we have
simulated ourselves. We’ll simulate data according to the model and
then check that we can sufficiently recover the parameter values used
in the simulation.</p>
<pre><code>data {
  int&lt;lower=1&gt; N;
  real&lt;lower=0&gt; mean_traps;
}
model {
} 
generated quantities {
  int traps[N];
  int complaints[N];
  real alpha = normal_rng(log(4), 0.1);
  real beta = normal_rng(-0.25, 0.1);
  
  for (n in 1:N)  {
    traps[n] = poisson_rng(mean_traps);
    complaints[n] = poisson_log_rng(alpha + beta * traps[n]);
  }
}</code></pre>
<p>We can use the <code>stan()</code> function to compile and fit the model, but
here we will do the compilation and fitting in two stages to make the
steps more explicit.</p>
<p>We can use the <code>stan()</code> function to compile and fit the model, but
here we will do the compilation and fitting in two stages to
demonstrate what is really happening under the hood.</p>
<p>First we will compile the Stan program
(<code>simple_poisson_regression_dgp.stan</code>) that will generate the fake
data.</p>
<p>Now we can simulate the data by calling the <code>sampling()</code> function.</p>
<pre><code>
SAMPLING FOR MODEL &#39;simple_poisson_regression_dgp&#39; NOW (CHAIN 1).
Chain 1: Iteration: 1 / 1 [100%]  (Sampling)
Chain 1:  Elapsed Time: 0 seconds (Warm-up)
Chain 1:                4.3e-05 seconds (Sampling)
Chain 1:                4.3e-05 seconds (Total)</code></pre>
<p>It is not necessary to supply the seed for the random number
generator; we do it here so that the code is fully reproducible: Same
seed, same random numbers, same output each time. (But we do not
guarantee identical output under future versions of Stan, as various
aspects of the program such as choice of starting points and
adaptation rules can change.)</p>
<p>We can now extract the sampled data and look at its structure in R:</p>
<pre><code>List of 5
 $ traps     : num [1, 1:120] 7 5 8 11 9 6 5 6 8 9 ...
  ..- attr(*, &quot;dimnames&quot;)=List of 2
  .. ..$ iterations: NULL
  .. ..$           : NULL
 $ complaints: num [1, 1:120] 0 1 0 0 0 0 0 0 1 0 ...
  ..- attr(*, &quot;dimnames&quot;)=List of 2
  .. ..$ iterations: NULL
  .. ..$           : NULL
 $ alpha     : num [1(1d)] 1.29
  ..- attr(*, &quot;dimnames&quot;)=List of 1
  .. ..$ iterations: NULL
 $ beta      : num [1(1d)] -0.283
  ..- attr(*, &quot;dimnames&quot;)=List of 1
  .. ..$ iterations: NULL
 $ lp__      : num [1(1d)] 0
  ..- attr(*, &quot;dimnames&quot;)=List of 1
  .. ..$ iterations: NULL</code></pre>
</div>
<div id="fitting-the-model-to-the-fake-data" class="section level3 unnumbered">
<h3>Fitting the model to the fake data</h3>
<p>In order to pass the fake data to our Stan program using RStan, we
need to arrange the data into a named list. The names must match the
names used in the <code>data</code> block of the Stan program.</p>
<pre><code>List of 3
 $ N         : int 120
 $ traps     : num [1:120] 7 5 8 11 9 6 5 6 8 9 ...
 $ complaints: num [1:120] 0 1 0 0 0 0 0 0 1 0 ...</code></pre>
<p>Now that we have the simulated data we fit the model to see if we can
recover the <code>alpha</code> and <code>beta</code> parameters used in the simulation.</p>
</div>
<div id="assessing-parameter-recovery" class="section level3 unnumbered">
<h3>Assessing parameter recovery</h3>
<p>We can compare the known values of the parameters to their posterior
distributions in the model fit to simulated data:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The posterior uncertainties are large here, but the true values are
well within the inferential ranges. If we did the simulation with many
more observations the parameters would be estimated much more
precisely while still including the true values (assuming the model
has been programmed correctly and the simulations have converged).</p>
<p>We should also check if the <code>y_rep</code> datasets (in-sample predictions)
that we coded in the <code>generated quantities</code> block are similar to the
<code>y</code> (complaints) values we conditioned on when fitting the model. (The
<strong>bayesplot</strong> package
<a href="http://mc-stan.org/bayesplot/articles/graphical-ppcs.html">vignettes</a>
are a good resource on this topic.)</p>
<p>Here is a plot of the density estimate of the observed data compared
to 200 of the <code>y_rep</code> datasets:
<img src="workflow_files/figure-html/unnamed-chunk-10-1.png" width="70%" style="display: block; margin: auto;" />
In the plot above we have the kernel density estimate of the observed
data (<span class="math inline">\(y\)</span>, thicker curve) and 200 simulated data sets (<span class="math inline">\(y_{rep}\)</span>, thin
curves) from the posterior predictive distribution. If the model fits
the data well, as it does here, there is little difference between the
observed dataset and the simulated datasets.</p>
<p>Another plot we can make for count data is a rootogram. This is a plot
of the expected counts (continuous line) vs the observed counts (blue
histogram). We can see the model fits well because the observed
histogram matches the expected counts relatively well.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-11-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="fiting-with-the-data-supplied-to-us" class="section level3 unnumbered">
<h3>Fiting with the data supplied to us</h3>
<p>To fit the model to the data given to us, we first create a list to
pass to Stan using the variables in the <code>pest_data</code> dataframe:</p>
<p>As we have already compiled the model, we can jump straight to
sampling from it.</p>
<p>and printing the parameters:</p>
<pre><code>Inference for Stan model: simple_poisson_regression.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

       mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
alpha  2.57       0 0.15  2.26  2.48  2.58  2.68  2.87  1044    1
beta  -0.19       0 0.02 -0.24 -0.21 -0.19 -0.18 -0.15  1038    1

Samples were drawn using NUTS(diag_e) at Thu Nov 29 19:08:38 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>The coefficient <span class="math inline">\(\beta\)</span> is estimated to be negative, impliying that a
higher number of bait stations set in a building appears to be
associated with fewer complaints about cockroaches in the following
month.</p>
<p>But we still need to consider how well the model fits.</p>
</div>
<div id="posterior-predictive-checking" class="section level3 unnumbered">
<h3>Posterior predictive checking</h3>
<p><img src="workflow_files/figure-html/marginal_PPC-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The replicated datasets are not as dispersed as the observed data and
don’t seem to capture the rate of zeroes in the observed data. The
Poisson model may not be a good fit for these data.</p>
<p>Let’s explore this further by looking directly at the proportion of
zeroes in the real data and predicted data.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-13-1.png" width="70%" style="display: block; margin: auto;" />
The plot above shows the observed proportion of zeroes (thick vertical
line) and a histogram of the proportion of zeroes in each of the
simulated data sets. It is clear that the model does not capture this
feature of the data well at all.</p>
<p>This next plot is a plot of the standardised residuals of the observed
vs predicted number of complaints.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-14-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>As you can see here, it looks as though we have more positive
residuals than negative, which indicates that the model tends to
underestimate the number of complaints that will be received.</p>
<p>The rootogram is another useful plot to compare the observed vs
expected number of complaints. This is a plot of the expected counts
(continuous line) vs the observed counts (blue histogram):</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>If the model was fitting well these would be relatively similar, but
this figure shows that the number of complaints is underestimated if
there are few complaints, over-estimated for medium numbers of
complaints, and underestimated if there are a large number of
complaints.</p>
<p>We can also view how the predicted number of complaints varies with
the number of bait stations. The model doesn’t seem to fully capture
the data.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-16-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Specifically, the model doesn’t capture the tails of the observed data
well.</p>
</div>
</div>
<div id="expanding-the-model-multiple-predictors" class="section level2">
<h2><span class="header-section-number">4.4</span> Expanding the model: multiple predictors</h2>
<p>Modeling the relationship between complaints and bait stations is the
simplest model. We can expand the model, however, in a few ways that
will be beneficial for our client. Moreover, the manager has told us
that they expect there are a number of other reasons that one building
might have more roach complaints than another.</p>
<div id="interpretability" class="section level3 unnumbered">
<h3>Interpretability</h3>
<p>Currently, our model’s mean parameter is a rate of complaints per 30
days, but we’re modeling a process that occurs over an area as well as
over time. We have the square footage of each building, so if we add
that information into the model, we can interpret our parameters as a
rate of complaints per square foot per 30 days.</p>
<p><span class="math display">\[ \begin{aligned}
\textrm{complaints}_{b,t} &amp; \sim \textrm{Poisson}(\textrm{sq}\_\textrm{foot}_b\,\lambda_{b,t}) \\
\lambda_{b,t} &amp; = \exp{(\eta_{b,t} )} \\
\eta_{b,t} &amp;= \alpha + \beta \, \textrm{traps}_{b,t}
\end{aligned} \]</span></p>
<p>The term <span class="math inline">\(\textrm{sq}\_\textrm{foot}\)</span> is called an exposure term. If we log the
term, we can put it in <span class="math inline">\(\eta_{b,t}\)</span>:</p>
<p><span class="math display">\[ \begin{aligned}
\textrm{complaints}_{b,t} &amp; \sim \textrm{Poisson}(\lambda_{b,t}) \\
\lambda_{b,t} &amp; = \exp{(\eta_{b,t} )} \\
\eta_{b,t} &amp;= \alpha + \beta \, \textrm{traps}_{b,t} + \textrm{log}\_\textrm{sq}\_\textrm{foot}_b
\end{aligned} \]</span></p>
<p>A quick check finds a relationship between the square footage of the
building and the number of complaints received:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-17-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Using the property manager’s intuition, we include two extra pieces of
information we know about the building - the (log of the) square floor
space and whether there is a live in super or not - into both the
simulated and real data.</p>
</div>
<div id="stan-program-for-poisson-multiple-regression" class="section level3 unnumbered">
<h3>Stan program for Poisson multiple regression</h3>
<p>Now we need a new Stan model that uses multiple predictors.</p>
<pre><code>functions {
  /*
  * Alternative to poisson_log_rng() that 
  * avoids potential numerical problems during warmup
  */
  int poisson_log_safe_rng(real eta) {
    real pois_rate = exp(eta);
    if (pois_rate &gt;= exp(20.79))
      return -9;
    return poisson_rng(pois_rate);
  }
}
data {
  int&lt;lower=1&gt; N;
  int&lt;lower=0&gt; complaints[N];
  vector&lt;lower=0&gt;[N] traps;
  vector&lt;lower=0,upper=1&gt;[N] live_in_super;
  vector[N] log_sq_foot;  // exposure term
}
parameters {
  real alpha;
  real beta;
  real beta_super;
}
model {
  beta ~ normal(-0.25, 1);
  beta_super ~ normal(-0.5, 1);
  alpha ~ normal(log(4), 1);  
  complaints ~ poisson_log(alpha + beta * traps + beta_super * live_in_super + log_sq_foot);
} 
generated quantities {
  int y_rep[N]; 
  for (n in 1:N) 
    y_rep[n] = poisson_log_safe_rng(alpha + beta * traps[n] + beta_super * live_in_super[n]
                                   + log_sq_foot[n]);
}</code></pre>
</div>
<div id="simulate-fake-data-with-multiple-predictors" class="section level3 unnumbered">
<h3>Simulate fake data with multiple predictors</h3>
<pre><code>
SAMPLING FOR MODEL &#39;multiple_poisson_regression_dgp&#39; NOW (CHAIN 1).
Chain 1: Iteration: 1 / 1 [100%]  (Sampling)
Chain 1:  Elapsed Time: 0 seconds (Warm-up)
Chain 1:                6.3e-05 seconds (Sampling)
Chain 1:                6.3e-05 seconds (Total)</code></pre>
<p>Now pop that simulated data into a list ready for Stan.</p>
<p>And then compile and fit the model we wrote for the multiple
regression.</p>
<p>Then compare these parameters to the true parameters:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-22-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Now that wee’ve recovered the parameters from the data we have
simulated, we’re ready to fit the data that were given to us.</p>
</div>
<div id="fit-the-data-given-to-us" class="section level3 unnumbered">
<h3>Fit the data given to us</h3>
<p>We explore the fit by comparing the data to posterior predictive
simulations:</p>
<p><img src="workflow_files/figure-html/fit_mult_P_real_dat-1.png" width="70%" style="display: block; margin: auto;" />
This again looks like we haven’t captured the smaller counts well, nor
have we captured the larger counts.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-23-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We’re still severely underestimating the proportion of zeroes in the
data. Ideally this vertical line would fall somewhere within the
histogram.</p>
<p>We can also plot uncertainty intervals for the predicted complaints for different
numbers of bait stations.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-24-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We’ve increased the tails a bit more at the larger numbers of bait
stations but we still have some large observed numbers of complaints
that the model would consider extremely unlikely events.</p>
</div>
</div>
<div id="modeling-count-data-with-the-negative-binomial-distribution" class="section level2">
<h2><span class="header-section-number">4.5</span> Modeling count data with the negative binomial distribution</h2>
<p>When we considered modelling the data using a Poisson, we saw that the
model didn’t appear to fit as well to the data as we would like. In
particular the model underpredicted low and high numbers of
complaints, and overpredicted the medium number of complaints. This is
one indication of overdispersion, where the variance is larger than
the mean. A Poisson model doesn’t fit overdispersed count data well
because the same parameter <span class="math inline">\(\lambda\)</span>, controls both the expected
counts and the variance of these counts. The natural alternative to
this is the negative binomial model:</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{complaints}_{b,t} &amp; \sim \mathrm{Neg-Binomial}(\lambda_{b,t}, \phi) \\
\lambda_{b,t} &amp; = \exp(\eta_{b,t}) \\
\eta_{b,t} &amp; = \alpha + \beta \, \mathrm{traps}_{b,t} + \beta_{\mathrm{super}} \, \mathrm{super}_{b} + \mathrm{log}\_\textrm{sq}\_\textrm{foot}_{b}
\end{aligned}
\]</span></p>
<p>In Stan the negative binomial mass function we’ll use has the signature</p>
<pre><code>neg_binomial_2_log(ints y, reals eta, reals phi)</code></pre>
<p>Like the <code>poisson_log</code> function, this negative binomial mass function
that is parameterized in terms of its log-mean, <span class="math inline">\(\eta\)</span>, but it also
has a precision <span class="math inline">\(\phi\)</span> such that</p>
<p><span class="math display">\[
\mbox{E}[y] \, = \lambda = \exp(\eta)
\]</span></p>
<p><span class="math display">\[
\text{Var}[y] = \lambda + \lambda^2/\phi = \exp(\eta) + \exp(\eta)^2 / \phi.
\]</span></p>
<p>As <span class="math inline">\(\phi\)</span> gets larger the term <span class="math inline">\(\lambda^2 / \phi\)</span> approaches zero and
so the variance of the negative-binomial approaches <span class="math inline">\(\lambda\)</span>; that
is, the negative-binomial gets closer and closer to the Poisson.</p>
<div id="stan-program-for-negative-binomial-regression" class="section level3 unnumbered">
<h3>Stan program for negative-binomial regression</h3>
<pre><code>functions {
  /*
  * Alternative to neg_binomial_2_log_rng() that 
  * avoids potential numerical problems during warmup
  */
  int neg_binomial_2_log_safe_rng(real eta, real phi) {
    real gamma_rate = gamma_rng(phi, phi / exp(eta));
    if (gamma_rate &gt;= exp(20.79))
      return -9;     
    return poisson_rng(gamma_rate);
  }
}
data {
  int&lt;lower=1&gt; N;
  vector&lt;lower=0&gt;[N] traps;
  vector&lt;lower=0,upper=1&gt;[N] live_in_super;
  vector[N] log_sq_foot;
  int&lt;lower=0&gt; complaints[N];
}
parameters {
  real alpha;
  real beta;
  real beta_super;
  real&lt;lower=0&gt; inv_phi;
}
transformed parameters {
  real phi = inv(inv_phi);
}
model {
  alpha ~ normal(log(4), 1);
  beta ~ normal(-0.25, 1);
  beta_super ~ normal(-0.5, 1);
  inv_phi ~ normal(0, 1); 
  complaints ~ neg_binomial_2_log(alpha + beta * traps + beta_super * live_in_super
                                  + log_sq_foot, phi);
} 
generated quantities {
  int y_rep[N];
  for (n in 1:N) 
    y_rep[n] = neg_binomial_2_log_safe_rng(alpha + beta * traps[n] +
      beta_super * live_in_super[n] + log_sq_foot[n], phi);
  
}</code></pre>
</div>
<div id="fake-data-fit-multiple-negative-binomial-regression" class="section level3 unnumbered">
<h3>Fake data fit: Multiple negative-binomial regression</h3>
<p>We’re going to generate one draw from the fake data model so we can
use the data to fit our model and compare the known values of the
parameters to the posterior density of the parameters.</p>
<p>Create a dataset to feed into the Stan model.</p>
<p>Compile the inferential model.</p>
<p>Now we run our NB regression over the fake data and extract the
samples to examine posterior predictive checks and to check whether
we’ve sufficiently recovered our known parameters, <span class="math inline">\(\text{alpha}\)</span>
<span class="math inline">\(\texttt{beta}\)</span>, .</p>
<p>Construct the vector of true values from your simulated dataset and
compare to the recovered parameters.
<img src="workflow_files/figure-html/unnamed-chunk-28-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="fiting-to-the-given-data-and-checking-the-fit" class="section level3 unnumbered">
<h3>Fiting to the given data and checking the fit</h3>
<p>Let’s look at our predictions vs. the data.</p>
<p><img src="workflow_files/figure-html/ppc-full-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>It appears that our model now captures both the number of small counts
better as well as the tails.</p>
<p>Let’s check if the negative binomial model does a better job capturing
the number of zeroes:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-29-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>These look OK, but let’s look at the standardized residual plot.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-30-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Looks OK, but we still have some large <em>standardized</em> residuals. This
might be because we are currently ignoring that the data are clustered
by buildings, and that the probability of roach issue may vary
substantially across buildings.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-31-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The rootogram now looks much more plausible. We can tell this because
now the expected number of complaints matches much closer to the
observed number of complaints. However, we still have some larger
counts that appear to be outliers for the model.</p>
<p>Check predictions by number of bait stations:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-32-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We haven’t used the fact that the data are clustered by building
yet. A posterior predictive check might elucidate whether it would be
a good idea to add the building information into the model.</p>
<p><img src="workflow_files/figure-html/ppc-group_means-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We’re getting plausible predictions for most building means but some
are estimated better than others and some have larger uncertainties
than we might expect. If we explicitly model the variation across
buildings we may be able to get much better estimates.</p>
</div>
</div>
<div id="hierarchical-modeling" class="section level2">
<h2><span class="header-section-number">4.6</span> Hierarchical modeling</h2>
<div id="modeling-varying-intercepts-for-each-building" class="section level3 unnumbered">
<h3>Modeling varying intercepts for each building</h3>
<p>Let’s add a hierarchical intercept parameter, <span class="math inline">\(\alpha_b\)</span> at the
building level to our model.</p>
<p><span class="math display">\[
\text{complaints}_{b,t} \sim \text{Neg-Binomial}(\lambda_{b,t}, \phi) \\
\lambda_{b,t}  = \exp{(\eta_{b,t})} \\
\eta_{b,t} = \mu_b + \beta \, {\rm traps}_{b,t} + \beta_{\rm super}\, {\rm super}_b + \text{log}\_\textrm{sq}\_\textrm{foot}_b \\
\mu_b \sim \text{normal}(\alpha, \sigma_{\mu})
\]</span></p>
<p>In our Stan model, <span class="math inline">\(\mu_b\)</span> is the <span class="math inline">\(b\)</span>-th element of the vector
<span class="math inline">\(\texttt{mu}\)</span> which has one element per building.</p>
<p>One of our predictors varies only by building, so we can rewrite the
above model more efficiently like so:</p>
<p><span class="math display">\[
\eta_{b,t} = \mu_b + \beta \, {\rm traps}_{b,t} + \text{log}\_\textrm{sq}\_\textrm{foot}_b\\
\mu_b \sim \text{normal}(\alpha +  \beta_{\text{super}} \, \text{super}_b , \sigma_{\mu})
\]</span></p>
<p>We have more information at the building level as well, like the
average age of the residents, the average age of the buildings, and
the average per-apartment monthly rent so we can add that data into a
matrix called <code>building_data</code>, which will have one row per building
and four columns:</p>
<ul>
<li><code>live_in_super</code></li>
<li><code>age_of_building</code></li>
<li><code>average_tentant_age</code></li>
<li><code>monthly_average_rent</code></li>
</ul>
<p>We’ll write the Stan model like:</p>
<p><span class="math display">\[
\eta_{b,t} = \alpha_b + \beta \, {\rm traps} + \text{log}\_\textrm{sq}\_\textrm{foot}\\
\mu \sim \text{normal}(\alpha + \texttt{building}\_\textrm{data} \, \zeta, \,\sigma_{\mu})
\]</span></p>
</div>
<div id="preparation-for-building-data-for-hierarchical-modeling" class="section level3 unnumbered">
<h3>Preparation for building data for hierarchical modeling</h3>
<p>We’ll need to do some more data prep before we can fit our
models. Firstly to use the building variable in Stan we will need to
transform it from a factor variable to an integer variable.</p>
</div>
<div id="compile-and-fit-the-hierarchical-model" class="section level3 unnumbered">
<h3>Compile and fit the hierarchical model</h3>
<p>Let’s compile the model.</p>
<p>Fit the model to data.</p>
</div>
<div id="diagnostics" class="section level3 unnumbered">
<h3>Diagnostics</h3>
<p>We get a bunch of warnings from Stan about divergent transitions,
which is an indication that there may be regions of the posterior that
have not been explored by the Markov chains.</p>
<p>Divergences are discussed in more detail in the course slides as well
as the <strong>bayesplot</strong> (MCMC diagnostics
vignette)[<a href="http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html" class="uri">http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html</a>]
and <a href="https://arxiv.org/abs/1701.02434"><em>A Conceptual Introduction to Hamiltonian Monte
Carlo</em></a>.</p>
<p>In this example we will see that we have divergent transitions because
we need to reparameterize our model. We will retain the overall
structure of the model but transform some of the parameters so that it
is easier for Stan to sample from the parameter space. Before we go
through exactly how to do this reparameterization, we will first go
through what indicates that this is something that reparameterization
will resolve. We will go through:</p>
<ol style="list-style-type: decimal">
<li>Examining the fitted parameter values, including the effective
sample size 2. Traceplots and scatterplots that reveal particular
patterns in locations of the divergences.</li>
</ol>
<p>First let’s extract the fits from the model.</p>
<p>Then we print the fits for the parameters that are of most interest.</p>
<pre><code>Inference for Stan model: hier_NB_regression.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

          mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
sigma_mu  0.26    0.01 0.16  0.06  0.14  0.23  0.34  0.67   967    1
beta     -0.23    0.00 0.06 -0.36 -0.27 -0.23 -0.19 -0.11   804    1
alpha     1.26    0.02 0.45  0.40  0.96  1.27  1.57  2.16   799    1
phi       1.58    0.01 0.34  1.01  1.34  1.53  1.76  2.39  3420    1
mu[1]     1.28    0.02 0.57  0.16  0.89  1.28  1.67  2.38   913    1
mu[2]     1.23    0.02 0.55  0.13  0.86  1.23  1.60  2.33   957    1
mu[3]     1.42    0.02 0.50  0.44  1.08  1.41  1.74  2.44   903    1
mu[4]     1.46    0.02 0.50  0.48  1.11  1.46  1.80  2.43   861    1
mu[5]     1.09    0.01 0.44  0.24  0.79  1.08  1.38  1.94  1123    1
mu[6]     1.17    0.02 0.50  0.18  0.83  1.17  1.50  2.14   859    1
mu[7]     1.47    0.02 0.54  0.42  1.11  1.47  1.83  2.52   898    1
mu[8]     1.26    0.01 0.43  0.42  0.97  1.25  1.55  2.14  1212    1
mu[9]     1.41    0.02 0.58  0.25  1.02  1.41  1.79  2.53   952    1
mu[10]    0.87    0.01 0.38  0.15  0.61  0.87  1.13  1.59  1110    1

Samples were drawn using NUTS(diag_e) at Thu Nov 29 19:12:39 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>You can see that the effective samples are low for many of the
parameters relative to the total number of samples. This alone isn’t
indicative of the need to reparameterize, but it indicates that we
should look further at the trace plots and pairs plots. First let’s
look at the traceplots to see if the divergent transitions form a
pattern.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-34-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Looks as if the divergent parameters, the little red bars underneath
the traceplots correspond to samples where the sampler gets stuck at
one parameter value for <span class="math inline">\(\sigma_\mu\)</span>.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-35-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>What we have here is a cloud-like shape, with most of the divergences
clustering towards the bottom. We’ll see a bit later that we actually
want this to look more like a funnel than a cloud, but the divergences
are indicating that the sampler can’t explore the narrowing neck of
the funnel.</p>
<p>One way to see why we should expect some version of a funnel is to
look at some simulations from the prior, which we can do without MCMC
and thus with no risk of sampling problems:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-36-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>If the data are at all informative we shouldn’t expect the posterior
to look exactly like the prior. But unless the data are highly
informative about the parameters and the posterior concentrates away
from the narrow neck of the funnel, the sampler is going to have to
confront the funnel geometry. (See the <a href="http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html">Visual MCMC
Diagnostics</a>
vignette for more on this.)</p>
<p>Another way to look at the divergences is via a parallel coordinates plot:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-37-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Again, we see evidence that our problems concentrate when <code>sigma_mu</code> is small.</p>
</div>
<div id="reparameterizing-and-rechecking-diagnostics" class="section level3 unnumbered">
<h3>Reparameterizing and rechecking diagnostics</h3>
<p>Instead, we should use the non-centered parameterization for
<span class="math inline">\(\mu_b\)</span>. We define a vector of auxiliary variables in the parameters
block, <code>mu_raw</code> that is given a <span class="math inline">\(\mathrm{normal}(0,1)\)</span> prior in the
model block. We then make <span class="math inline">\(\texttt{mu}\)</span> a transformed parameter: We
can reparameterize the random intercept <span class="math inline">\(\mu_b\)</span>, which is distributed:</p>
<p><span class="math display">\[
\mu_b \sim \text{normal}(\alpha + \texttt{building}\_\textrm{data} \, \zeta,
                         \sigma_{\mu})
\]</span></p>
<pre><code>transformed parameters {
  vector[J] mu;
  mu = alpha + building_data * zeta + sigma_mu * mu_raw;
}</code></pre>
<p>This gives <span class="math inline">\(\texttt{mu}\)</span> a <span class="math inline">\(\text{normal}(\alpha + \texttt{building}\_\textrm{data}\, \zeta, \sigma_\mu)\)</span> distribution, but it
decouples the dependence of the density of each element of
<span class="math inline">\(\texttt{mu}\)</span> from <code>sigma_mu</code> (<span class="math inline">\(\sigma_\mu\)</span>).
hier_NB_regression_ncp.stan uses the non-centered parameterization for
<span class="math inline">\(\texttt{mu}\)</span>. We will examine the effective sample size of the fitted
model to see whether we’ve fixed the problem with our
reparameterization.</p>
<p>Compile the model.</p>
<p>Fit the model to the data.</p>
<p>Examining the fit of the new model</p>
<pre><code>Inference for Stan model: hier_NB_regression_ncp.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

          mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
sigma_mu  0.22    0.01 0.17  0.01  0.10  0.19  0.31  0.66  1075    1
beta     -0.23    0.00 0.06 -0.35 -0.27 -0.23 -0.19 -0.10  2036    1
alpha     1.25    0.01 0.44  0.37  0.97  1.25  1.54  2.13  2093    1
phi       1.59    0.01 0.36  1.01  1.33  1.54  1.79  2.40  4615    1
mu[1]     1.27    0.01 0.55  0.17  0.93  1.27  1.63  2.34  2103    1
mu[2]     1.22    0.01 0.54  0.13  0.87  1.22  1.57  2.28  2129    1
mu[3]     1.39    0.01 0.50  0.41  1.05  1.39  1.72  2.40  2505    1
mu[4]     1.42    0.01 0.49  0.47  1.10  1.41  1.75  2.39  2406    1
mu[5]     1.07    0.01 0.42  0.26  0.80  1.07  1.35  1.91  2398    1
mu[6]     1.18    0.01 0.49  0.20  0.85  1.19  1.51  2.13  2176    1
mu[7]     1.46    0.01 0.53  0.41  1.11  1.45  1.79  2.52  2372    1
mu[8]     1.23    0.01 0.44  0.37  0.93  1.22  1.53  2.11  2704    1
mu[9]     1.42    0.01 0.58  0.28  1.04  1.42  1.79  2.54  2074    1
mu[10]    0.86    0.01 0.37  0.17  0.61  0.85  1.10  1.62  2822    1

Samples were drawn using NUTS(diag_e) at Thu Nov 29 19:13:39 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p>This has improved the effective sample sizes of <span class="math inline">\(\texttt{mu}\)</span>. We
extract the parameters to run our usual posterior predictive checks.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-38-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="workflow_files/figure-html/unnamed-chunk-39-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The marginal plot, again:</p>
<p><img src="workflow_files/figure-html/ppc-full-hier-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This looks good. If we’ve captured the building-level means well, then
the posterior distribution of means by building should match well with
the observed means of the quantity of building complaints by month.</p>
<p><img src="workflow_files/figure-html/ppc-group_means-hier-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We weren’t doing terribly with the building-specific means before, but
now they are all well captured by our model. The model is also able to
do a decent job estimating within-building variability:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-40-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Predictions by number of bait stations:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-41-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Standardized residuals:
<img src="workflow_files/figure-html/unnamed-chunk-42-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Rootogram:
<img src="workflow_files/figure-html/unnamed-chunk-43-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="varying-intercepts-and-varying-slopes" class="section level3 unnumbered">
<h3>Varying intercepts and varying slopes</h3>
<p>We’ve gotten some new data that extends the number of time points for
which we have observations for each building. This will let us explore
how to expand the model a bit more with varying <em>slopes</em> in addition
to the varying intercepts and also, later, also model temporal
variation.</p>
<p>Perhaps if the levels of complaints differ by building, so does the
coefficient for the effect of bait stations. We can add these varying
coefficients to our model and observe the fit.</p>
<p><span class="math display">\[
\text{complaints}_{b,t} \sim \text{Neg-Binomial}(\lambda_{b,t}, \phi)
\\
\lambda_{b,t} = \exp{(\eta_{b,t})}
\\
\eta_{b,t} = \mu_b + \kappa_b \, \texttt{traps}_{b,t}
             + \text{log}\_\textrm{sq}\_\textrm{foot}_b
\\
\mu_b \sim \text{normal}(\alpha + \texttt{building}\_\textrm{data} \, \zeta,
                         \sigma_{\mu}) \\
\kappa_b \sim \text{normal}(\beta + \texttt{building}\_\textrm{data} \, \gamma,
                            \sigma_{\kappa})
\]</span></p>
<p>Let’s compile the model.</p>
<p>Fit the model to data and extract the posterior draws needed for
our posterior predictive checks.</p>
<p>To see if the model infers building-to-building differences in, we can
plot a histogram of our marginal posterior distribution for
<code>sigma_kappa</code>.</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-45-1.png" width="70%" style="display: block; margin: auto;" /></p>
<pre><code>Inference for Stan model: hier_NB_regression_ncp_slopes_mod.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean   sd  2.5%   25%   50%   75%   98% n_eff Rhat
kappa[1]    -0.02    0.00 0.08 -0.14 -0.07 -0.03  0.03  0.16  1178    1
kappa[2]    -0.42    0.00 0.10 -0.63 -0.48 -0.41 -0.35 -0.24  1531    1
kappa[3]    -0.59    0.00 0.10 -0.79 -0.65 -0.59 -0.52 -0.39  4990    1
kappa[4]    -0.22    0.00 0.07 -0.36 -0.26 -0.22 -0.18 -0.08  3892    1
kappa[5]    -0.60    0.00 0.09 -0.79 -0.66 -0.60 -0.54 -0.43  4207    1
kappa[6]    -0.43    0.00 0.11 -0.67 -0.49 -0.43 -0.36 -0.23  2834    1
kappa[7]    -0.31    0.00 0.07 -0.44 -0.35 -0.31 -0.27 -0.18  5794    1
kappa[8]    -0.23    0.00 0.15 -0.56 -0.32 -0.22 -0.13  0.05  2111    1
kappa[9]     0.08    0.00 0.06 -0.03  0.04  0.08  0.12  0.20  4785    1
kappa[10]   -0.72    0.00 0.16 -1.01 -0.82 -0.73 -0.62 -0.38  1367    1
beta        -0.34    0.00 0.06 -0.47 -0.38 -0.34 -0.31 -0.22  2907    1
alpha        1.40    0.01 0.31  0.73  1.21  1.42  1.60  1.99  3092    1
phi          1.61    0.00 0.19  1.27  1.48  1.60  1.73  2.02  4282    1
sigma_mu     0.50    0.02 0.41  0.01  0.18  0.40  0.73  1.50   597    1
sigma_kappa  0.13    0.00 0.09  0.02  0.07  0.10  0.16  0.35   591    1
mu[1]        0.27    0.02 0.73 -1.43 -0.13  0.37  0.77  1.46  1185    1
mu[2]        1.65    0.01 0.54  0.68  1.29  1.61  1.97  2.84  1508    1
mu[3]        2.13    0.00 0.33  1.50  1.90  2.12  2.34  2.77  5320    1
mu[4]        1.47    0.01 0.52  0.45  1.15  1.47  1.80  2.54  3998    1
mu[5]        2.39    0.01 0.42  1.59  2.10  2.38  2.68  3.24  4499    1
mu[6]        1.89    0.01 0.41  1.17  1.64  1.86  2.11  2.81  2692    1
mu[7]        2.68    0.00 0.25  2.21  2.51  2.67  2.85  3.18  5405    1
mu[8]       -0.53    0.02 0.97 -2.33 -1.17 -0.57  0.06  1.47  2145    1
mu[9]        0.22    0.01 0.57 -0.87 -0.16  0.22  0.60  1.35  4682    1
mu[10]       1.81    0.03 1.08 -0.67  1.21  1.94  2.57  3.59   999    1

Samples were drawn using NUTS(diag_e) at Thu Nov 29 19:16:16 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p><img src="workflow_files/figure-html/unnamed-chunk-47-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>While the model can’t specifically rule out zero from the posterior,
it does have mass at small non-zero numbers, so we should leave in the
hierarchy over <span class="math inline">\(\texttt{kappa}\)</span>. Plotting the marginal data density
again, the model still looks well calibrated.</p>
<p><img src="workflow_files/figure-html/ppc-full-hier-slopes-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="time-varying-effects-and-structured-priors" class="section level2">
<h2><span class="header-section-number">4.7</span> Time-varying effects and structured priors</h2>
<p>We haven’t looked at how cockroach complaints change over time. Let’s
look at whether there’s any pattern over time.</p>
<p><img src="workflow_files/figure-html/ppc-group_max-hier-slopes-mean-by-mo-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We might augment our model with a log-additive monthly effect,
<span class="math inline">\(\texttt{mo}_t\)</span>.</p>
<p><span class="math display">\[
\eta_{b,t}
  = \mu_b + \kappa_b \, \texttt{traps}_{b,t}
    + \texttt{mo}_t + \text{log}\_\textrm{sq}\_\textrm{foot}_b
\]</span></p>
<p>We have complete freedom over how to specify the prior for
<span class="math inline">\(\texttt{mo}_t\)</span>. There are several competing factors for how the
number of complaints might change over time. It makes sense that there
might be more roaches in the environment during the summer, but we
might also expect that there is more roach control in the summer as
well. Given that we’re modeling complaints, maybe after the first
sighting of roaches in a building, residents are more vigilant, and
thus complaints of roaches would increase.</p>
<p>This can be a motivation for using an autoregressive prior for our
monthly effects. The model is as follows:</p>
<p><span class="math display">\[
\texttt{mo}_t
  \sim \text{normal}(\rho \, \texttt{mo}_{t-1}, \sigma_\texttt{mo}) \\
\equiv
\\
\texttt{mo}_t
= \rho \, \texttt{mo}_{t-1} + \epsilon_t , \quad \epsilon_t
  \sim \text{normal}(0, \sigma_\texttt{mo})
\\
\quad \rho \in [-1,1]
\]</span></p>
<p>This equation says that the monthly effect in month <span class="math inline">\(t\)</span> is directly
related to the last month’s monthly effect. Given the description of
the process above, it seems like there could be either positive or
negative associations between the months, but there should be a bit
more weight placed on positive <span class="math inline">\(\rho\)</span>s, so we’ll put an informative
prior that pushes the parameter <span class="math inline">\(\rho\)</span> towards 0.5.</p>
<p>Before we write our prior, however, we have a problem: Stan doesn’t
implement any densities that have support on <span class="math inline">\([-1,1]\)</span>. We can use
variable transformation of a raw variable defined on <span class="math inline">\([0,1]\)</span> to to
give us a density on <span class="math inline">\([-1,1]\)</span>. Specifically,</p>
<p><span class="math display">\[
\rho_{\text{raw}} \in [0, 1]
\\
\rho = 2 * \rho_{\text{raw}} - 1
\]</span></p>
<p>Then we can put a beta prior on <span class="math inline">\(\rho_\text{raw}\)</span> to push our estimate
towards 0.5.</p>
<p>One further wrinkle is that we have a prior for <span class="math inline">\(\texttt{mo}_t\)</span> that
depends on <span class="math inline">\(\texttt{mo}_{t-1}\)</span>. That is, we are working with the
<em>conditional</em> distribution of <span class="math inline">\(\texttt{mo}_t\)</span> given
<span class="math inline">\(\texttt{mo}_{t-1}\)</span>. But what should we do about the prior for
<span class="math inline">\(\texttt{mo}_1\)</span>, for which we don’t have a previous time period in the
data?</p>
<p>We need to work out the <em>marginal</em> distribution of the first
observation. Thankfully we can use the fact that AR models are
stationary, so <span class="math inline">\(\text{Var}(\texttt{mo}_t) = \text{Var}(\texttt{mo}_{t-1})\)</span> and <span class="math inline">\(\mbox{E}(\texttt{mo}_t) = \mbox{E}(\texttt{mo}_{t-1})\)</span> for all <span class="math inline">\(t\)</span>. Therefore the marginal
distribution of <span class="math inline">\(\texttt{mo}_1\)</span> is the same as the marginal
distribution of any <span class="math inline">\(\texttt{mo}_t\)</span>.</p>
<p>First we derive the marginal variance of <span class="math inline">\(\texttt{mo}_{t}\)</span>.</p>
<p><span class="math display">\[
\text{Var}(\texttt{mo}_t)
  = \text{Var}(\rho \texttt{mo}_{t-1} + \epsilon_t)
\\
\text{Var}(\texttt{mo}_t)
  = \text{Var}(\rho \texttt{mo}_{t-1}) + \text{Var}(\epsilon_t)
\]</span>
where the second line holds by independence of <span class="math inline">\(\epsilon_t\)</span> and
<span class="math inline">\(\epsilon_{t-1})\)</span>. Then, using the fact that <span class="math inline">\(Var(cX) = c^2Var(X)\)</span>
for a constant <span class="math inline">\(c\)</span> and the fact that, by stationarity,
<span class="math inline">\(\textrm{Var}(\texttt{mo}_{t-1}) = \textrm{Var}(\texttt{mo}_{t})\)</span>, we
then obtain:</p>
<p><span class="math display">\[
\text{Var}(\texttt{mo}_t)
  = \rho^2 \text{Var}( \texttt{mo}_{t})  + \sigma_\texttt{mo}^2
\\
\text{Var}(\texttt{mo}_t)
  = \frac{\sigma_\texttt{mo}^2}{1 - \rho^2}
\]</span></p>
<p>For the mean of <span class="math inline">\(\texttt{mo}_t\)</span> things are a bit simpler:</p>
<p><span class="math display">\[
\mbox{E}(\texttt{mo}_t)
  = \mbox{E}(\rho \, \texttt{mo}_{t-1} + \epsilon_t)
\\
\mbox{E}(\texttt{mo}_t)
  = \mbox{E}(\rho \, \texttt{mo}_{t-1}) + \mbox{E}(\epsilon_t)
\]</span></p>
<p>Since <span class="math inline">\(\mbox{E}(\epsilon_t) = 0\)</span> by assumption we have</p>
<p><span class="math display">\[
\mbox{E}(\texttt{mo}_t) = \mbox{E}(\rho \, \texttt{mo}_{t-1})  + 0\\
\mbox{E}(\texttt{mo}_t) = \rho \, \mbox{E}(\texttt{mo}_{t}) \\
\mbox{E}(\texttt{mo}_t) - \rho \mbox{E}(\texttt{mo}_t) = 0  \\
\mbox{E}(\texttt{mo}_t) = 0/(1 - \rho)
\]</span></p>
<p>which for <span class="math inline">\(\rho \neq 1\)</span> yields <span class="math inline">\(\mbox{E}(\texttt{mo}_{t}) = 0\)</span>.</p>
<p>We now have the marginal distribution for <span class="math inline">\(\texttt{mo}_{t}\)</span>, which, in
our case, we will use for <span class="math inline">\(\texttt{mo}_1\)</span>. The full AR(1)
specification is then:</p>
<p><span class="math display">\[
\texttt{mo}_1 \sim
\text{normal}\left(0, \frac{\sigma_\texttt{mo}}{\sqrt{1 - \rho^2}}\right)
\\
\texttt{mo}_t \sim
\text{normal}\left(\rho \, \texttt{mo}_{t-1}, \sigma_\texttt{mo}\right)
\ \forall t &gt; 1
\]</span></p>
<p>In the interest of brevity, we won’t go on expanding the model, though
we certainly could. What other information would help us understand
the data generating process better? What other aspects of the data
generating process might we want to capture that we’re not capturing
now?</p>
<p>As usual, we run through our posterior predictive checks.</p>
<p><img src="workflow_files/figure-html/ppc-full-hier-mos-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="workflow_files/figure-html/unnamed-chunk-48-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Our monthly random intercept has captured a monthly pattern across all
the buildings. We can also compare the prior and posterior for the
autoregressive parameter to see how much we’ve learned. Here are two
different ways of comparing the prior and posterior visually:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-49-1.png" width="70%" style="display: block; margin: auto;" /><img src="workflow_files/figure-html/unnamed-chunk-49-2.png" width="70%" style="display: block; margin: auto;" /></p>
<pre><code>Inference for Stan model: hier_NB_regression_ncp_slopes_mod_mos.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

             mean se_mean   sd  2.5%   25%   50%   75%  98% n_eff Rhat
rho          0.78    0.00 0.08  0.59  0.73  0.78  0.83 0.91  1361    1
sigma_mu     0.31    0.01 0.24  0.01  0.12  0.27  0.44 0.93  1389    1
sigma_kappa  0.09    0.00 0.05  0.01  0.05  0.08  0.11 0.22   958    1
gamma[1]    -0.18    0.00 0.11 -0.40 -0.25 -0.18 -0.12 0.03  2085    1
gamma[2]     0.12    0.00 0.07 -0.03  0.07  0.11  0.16 0.27  1913    1
gamma[3]     0.11    0.00 0.15 -0.18  0.02  0.10  0.19 0.40  2210    1
gamma[4]     0.00    0.00 0.06 -0.13 -0.04  0.00  0.03 0.12  2334    1

Samples were drawn using NUTS(diag_e) at Thu Nov 29 19:19:02 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p><img src="workflow_files/figure-html/unnamed-chunk-51-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>It looks as if our model finally generates a reasonable posterior
predictive distribution for all numbers of bait stations, and
appropriately captures the tails of the data generating process.</p>
</div>
<div id="using-our-model-cost-forecasts" class="section level2">
<h2><span class="header-section-number">4.8</span> Using our model: Cost forecasts</h2>
<p>Our model seems to be fitting well, so now we will go ahead and use
the model to help us make a decision about how many bait stations to
put in our buildings. We’ll make a forecast for 6 months forward.</p>
<p>An important input to the revenue model is how much revenue is lost
due to each complaint. The client has a policy that for every 10
complaints, they’ll call an exterminator costing the client $100, so
that’ll amount to $10 per complaint.</p>
<p>Below we’ve generated revenue curves for the buildings. These charts
will give us precise quantification of our uncertainty around our
revenue projections at any number of bait stations for each building.</p>
<p>A key input to our analysis will be the cost of installing bait
stations. We’re simulating the number of complaints we receive over
the course of a year, so we need to understand the cost associated
with maintaining each bait station over the course of a year. There’s
the cost attributed to the raw bait station, which is the plastic
housing and the bait material, a peanut-buttery substance that’s
injected with insecticide. The cost of maintaining one bait station
for a year plus monthly replenishment of the bait material is about
$20.</p>
<p>We’ll also need labor for maintaining the bait stations, which need to
be serviced every two months. If there are fewer than five bait
stations, our in-house maintenance staff can manage the stations
(about one hour of work every two months at $20/hour), but above five
bait stations we need to hire outside pest control to help
out. They’re a bit more expensive, so we’ve put their cost at $30 /
hour. Each five bait stations should require an extra person-hour of
work, so that’s factored in as well. The marginal person-person hours
above five bait stations are at the higher pest-control labor rate.</p>
<p>We can now plot curves with number of bait stations on the x-axis and
profit/loss forecasts and uncertainty intervals on the y-axis.</p>
<p><img src="workflow_files/figure-html/rev-curves-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can can see that the optimal number of bait stations differs by
building.</p>
<p><br></p>
<p>Left as an exercise for the reader:</p>
<ul>
<li><p>How would we build a revenue curve for a new building?</p></li>
<li><p>Let’s say our utility function is revenue. If we wanted to maximize
expected revenue, we can take expectations at each station count for
each building, and choose the trap numbers that maximizes expected
revenue. This will be called a maximum revenue strategy. How can we
generate the distribution of portfolio revenue (the sum of revenue
across all the buildings) under the maximum revenue strategy from the
the draws of <code>rev_pred</code> we already have?</p></li>
</ul>
</div>
<div id="gaussian-process-instead-of-ar1" class="section level2">
<h2><span class="header-section-number">4.9</span> Gaussian process instead of AR(1)</h2>
<div id="joint-density-for-ar1-process" class="section level3 unnumbered">
<h3>Joint density for AR(1) process</h3>
<p>We can derive the joint distribution for the AR(1) process before we
move to the Gaussian process (GP) which will give us a little more
insight into what a GP is. Remember that we’ve specified the AR(1)
prior as:</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{mo}_1
&amp; \sim
  \text{normal}\left(0, \frac{\sigma_\texttt{mo}}{\sqrt{1 - \rho^2}}\right)
\\
\texttt{mo}_t
&amp; \sim
  \text{normal}\left(\rho \, \texttt{mo}_{t-1}, \sigma_\texttt{mo}\right)
  \forall t &gt; 1
\end{aligned}
\]</span></p>
<p>Rewriting our process in terms of the errors will make the derivation
of the joint distribution clearer</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{mo}_1
&amp; \sim
  \text{normal}\left(0, \frac{\sigma_\texttt{mo}}{\sqrt{1 - \rho^2}}\right)
\\
\texttt{mo}_t
&amp; =
  \rho \, \texttt{mo}_{t-1} + \sigma_\texttt{mo}\epsilon_t
\\
\epsilon_t
&amp; \sim
  \text{normal}\left(0, 1\right)
\end{aligned}
\]</span></p>
<p>Given that the first term <span class="math inline">\(\texttt{mo}_1\)</span> is normally distributed, and
the other terms are sums of normal random variables, jointly the
vector, <code>mo</code>, with the <span class="math inline">\(t\)</span>-th element equally the scalar
<span class="math inline">\(\texttt{mo}_t\)</span>, is multivariate normal, with mean zero (which we
derived above). More formally, if we have a vector <span class="math inline">\(x \in \mathbb{R}^M\)</span> which is multivariate normal,
<span class="math inline">\(x \sim \text{multiviarate normal}(0, \Sigma)\)</span>
and we left-multiply <span class="math inline">\(x\)</span> by a nonsingular matrix
<span class="math inline">\(L \in \mathbb{R}^{M\times M}\)</span>,
<span class="math inline">\(y = Lx \sim \text{multivariate normal}(0, L\Sigma L^T)\)</span>.
We can use this fact to show that our
vector <code>mo</code> is jointly multivariate normal.</p>
<p>Just as before with the noncentered parameterization, we’ll be taking
a vector <span class="math inline">\(\texttt{mo}\_\textrm{raw} \in \mathbb{R}^M\)</span> in which each element is
univariate <span class="math inline">\(\text{normal}(0,1)\)</span> and transforming it into <code>mo</code>, but
instead of doing the transformation with scalar transformations as in
the section <strong>Time varying effects and structured priors</strong>, we’ll do
it with linear algebra operations. The trick is that by specifying
each element of <span class="math inline">\(\texttt{mo}\_\textrm{raw}\)</span> to be distributed
<span class="math inline">\(\text{normal}(0,1)\)</span> we are implicitly defining
<span class="math inline">\(\texttt{mo}\_\textrm{raw} \sim \text{multivariate normal}(0, I_M)\)</span>,
where <span class="math inline">\(I_M\)</span> is the identity matrix of dimension <span class="math inline">\(M \times M\)</span>.
Then we do a linear transformation
using a matrix <span class="math inline">\(L\)</span> and assign the result to <code>mo</code> like
<span class="math inline">\(\texttt{mo} = L*\texttt{mo}\_\textrm{raw}\)</span> so
<span class="math inline">\(\texttt{mo} \sim \text{multivariate normal}(0, LI_M L^T)\)</span>
and <span class="math inline">\(LI_M L^T = LL^T\)</span>.</p>
<p>Consider the case where we have three elements in <code>mo</code> and we want to
make figure out the form for <span class="math inline">\(L\)</span>.</p>
<p>The first element of <code>mo</code> is fairly straightforward, because it
mirrors our earlier parameterization of the AR(1) prior. The only
difference is that we’re explicitly adding the last two terms of
<code>mo_raw</code> into the equation so we can use matrix algebra for our
transformation.
<span class="math display">\[
\texttt{mo}_1
  = \frac{\sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}}
      \times \texttt{mo}\_\textrm{raw}_1
    + 0 \times \texttt{mo}\_\textrm{raw}_2
    + 0 \times \texttt{mo}\_\textrm{raw}_3
\]</span></p>
<p>The second element is a bit more complicated:</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{mo}_2
&amp; =
\rho \texttt{mo}_1
  + \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2
  + 0 \times \texttt{mo}\_\textrm{raw}_3
\\
&amp; =
\rho \left( \frac{\sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}}
            \times \texttt{mo}\_\textrm{raw}_1 \right)
  + \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2
  + 0 \times \texttt{mo}\_\textrm{raw}_3
\\[5pt]
&amp; =
\frac{\rho \sigma_{\texttt{mo}}}
     {\sqrt{1 - \rho^2}} \times \texttt{mo}\_\textrm{raw}_1
  + \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2
  + 0 \times \texttt{mo}\_\textrm{raw}_3
\end{aligned}
\]</span></p>
<p>While the third element will involve all three terms</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{mo}_3
&amp; = \rho \, \texttt{mo}_2
    + \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_3
\\
&amp; = \rho \left( \frac{\rho \sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}}
                  \times\texttt{mo}\_\textrm{raw}_1
        + \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2 \right)
    + \sigma_{\texttt{mo}} \texttt{mo}\_\textrm{raw}_3
\\[5pt]
&amp; = \frac{\rho^2 \sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}}
      \times \texttt{mo}\_\textrm{raw}_1
    + \rho \, \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2
    +  \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_3
\end{aligned}
\]</span></p>
<p>Writing this all together:</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{mo}_1
&amp; = \frac{\sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}} \times \texttt{mo}\_\textrm{raw}_1
    + 0 \times \texttt{mo}\_\textrm{raw}_2
    + 0 \times \texttt{mo}\_\textrm{raw}_3
\\[3pt]
\texttt{mo}_2
&amp; = \frac{\rho \sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}}
      \times \texttt{mo}\_\textrm{raw}_1
    + \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2
    + 0 \times \texttt{mo}\_\textrm{raw}_3
\\[3pt]
\texttt{mo}_3
&amp; = \frac{\rho^2 \sigma_{\texttt{mo}}}{\sqrt{1 - \rho^2}}
      \times \texttt{mo}\_\textrm{raw}_1
    + \rho \, \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_2
    +  \sigma_{\texttt{mo}}\,\texttt{mo}\_\textrm{raw}_3
\end{aligned}
\]</span></p>
<p>Separating this into a matrix of coefficients <span class="math inline">\(L\)</span> and the vector <code>mo_raw</code>:</p>
<p><span class="math display">\[
\texttt{mo}
=
\begin{bmatrix}
\sigma_\texttt{mo} / \sqrt{1 - \rho^2} &amp; 0 &amp; 0
\\
\rho \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; \sigma_\texttt{mo} &amp; 0
\\
\rho^2 \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; \rho \,\sigma_\texttt{mo}
&amp; \sigma_\texttt{mo}
\end{bmatrix}
  \times \texttt{mo}\_\textrm{raw}
\]</span></p>
<p>If we multiply <span class="math inline">\(L\)</span> on the right by its transpose <span class="math inline">\(L^T\)</span>, we’ll get
expressions for the covariance matrix of our multivariate random
vector <code>mo</code>:</p>
<p><span class="math display">\[
\begin{bmatrix} \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; 0
&amp; 0
\\
\rho \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; \sigma_\texttt{mo}
&amp; 0
\\
\rho^2 \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; \rho \,\sigma_\texttt{mo}
&amp; \sigma_\texttt{mo}
\end{bmatrix}
\times
\begin{bmatrix}
\sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; \rho \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
&amp; \rho^2 \sigma_\texttt{mo} / \sqrt{1 - \rho^2}
\\
0 &amp; \sigma_\texttt{mo} &amp; \rho \,\sigma_\texttt{mo}
\\
0 &amp; 0  &amp; \sigma_\texttt{mo}
\end{bmatrix}
\]</span></p>
<p>which results in:</p>
<p><span class="math display">\[
\begin{bmatrix}
\sigma^2_\texttt{mo}
  / (1 - \rho^2) &amp; \rho \, \sigma^2_\texttt{mo} / (1 - \rho^2)
&amp;  \rho^2 \, \sigma^2_\texttt{mo} / (1 - \rho^2)
\\
\rho \, \sigma^2_\texttt{mo} / (1 - \rho^2)
&amp; \sigma^2_\texttt{mo}
    / (1 - \rho^2)  &amp; \rho \, \sigma^2_\texttt{mo} / (1 - \rho^2)
\\
\rho^2 \sigma^2_\texttt{mo} / (1 - \rho^2)
&amp; \rho \, \sigma^2_\texttt{mo}
  / (1 - \rho^2) &amp; \sigma^2_\texttt{mo} / (1 - \rho^2)
\end{bmatrix}
\]</span>
We can simplify this result by dividing the matrix by
<span class="math inline">\(\sigma^2_\texttt{mo} / (1 - \rho^2)\)</span> to get</p>
<p><span class="math display">\[
\begin{bmatrix} 1 &amp; \rho  &amp;  \rho^2 \\
\rho  &amp; 1  &amp; \rho  \\
\rho^2 &amp; \rho &amp; 1
\end{bmatrix}
\]</span></p>
<p>This should generalize to higher dimensions pretty easily. We could
replace the Stan code in lines 59 to 63 in
<code>stan/hier_NB_regression_ncp_slopes_mod_mos.stan</code> with the following:</p>
<pre><code>vector[M] mo;
{
  matrix[M,M] A = rep_matrix(0, M, M);
  A[1,1] = sigma_mo / sqrt(1 - rho^2);
  for (m in 2:M)
    A[m,1] = rho^(m-1) * sigma_mo / sqrt(1 - rho^2);
  for (m in 2:M) {
    A[m,m] = sigma_mo;
    for (i in (m + 1):M)
      A[i,m] = rho^(i-m) * sigma_mo;
  }
  mo = A * mo_raw;
}</code></pre>
<p>The existing Stan code in lines 59 to 63 is doing the exact same
calculations but more efficiently.</p>
</div>
<div id="cholesky-decomposition" class="section level3 unnumbered">
<h3>Cholesky decomposition</h3>
<p>If we only knew the covariance matrix of our process, say a matrix
called <span class="math inline">\(\Sigma\)</span>, and we had a way of decomposing <span class="math inline">\(\Sigma\)</span> into <span class="math inline">\(L L^T\)</span>, then we wouldn’t need to write out the equation for the
vector. Luckily, there is a matrix decomposition called the <strong>Cholesky
decomposition</strong> that does just that. The Stan function for the
composition is called <code>cholesky_decompose</code>. Instead of writing out the
explicit equation, we could do the following:</p>
<pre><code>vector[M] mo;
{
  mo = cholesky_decompose(Sigma) * mo_raw;
}</code></pre>
<p>provided we’ve defined <code>Sigma</code> appropriately elsewhere in the
transformed parameter block. The matrix <span class="math inline">\(L\)</span> is lower triangular; that
is, all elements in the upper right triangle of the matrix are zero.</p>
<p>We’ve already derived the covariance matrix <code>Sigma</code> for the
three-dimensional AR(1) process above by explicitly calculating <span class="math inline">\(L L^T\)</span>, but we can do so using the rules of covariance and the way our
process is defined. We already know that each element of
<span class="math inline">\(\texttt{mo}_t\)</span> has marginal variance <span class="math inline">\(\sigma^2_\texttt{mo} / (1 - \rho^2)\)</span>, but we don’t know the covariance of <span class="math inline">\(\texttt{mo}_t\)</span> and
<span class="math inline">\(\texttt{mo}_{t+h}\)</span>. We can do so recursively. First we derive the
covariance for two elements of <span class="math inline">\(\texttt{mo}_t\)</span> separated by one month:</p>
<p><span class="math display">\[
\text{Cov}(\texttt{mo}_{t+1},\texttt{mo}_{t})
  = \text{Cov}(\rho \, \texttt{mo}_{t}
     + \sigma_\texttt{mo}\epsilon_{t+1},\texttt{mo}_{t})
\\
\text{Cov}(\texttt{mo}_{t+1},\texttt{mo}_{t})
  = \rho \text{Cov}(\texttt{mo}_{t},\texttt{mo}_{t})
    + \sigma_\texttt{mo}\text{Cov}(\epsilon_{t+1},\texttt{mo}_{t})
\\
\text{Cov}(\texttt{mo}_{t+1},\texttt{mo}_{t})
  = \rho \text{Var}(\texttt{mo}_t)
     + 0
\]</span></p>
<p>Then we define the covariance for
<span class="math inline">\(\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})\)</span> in terms of
<span class="math inline">\(\text{Cov}(\texttt{mo}_{t+h-1},\texttt{mo}_{t})\)</span></p>
<p><span class="math display">\[
\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})
  = \text{Cov}(\rho \, \texttt{mo}_{t+h-1}
    + \sigma_\texttt{mo}\epsilon_{t+h},\texttt{mo}_{t})
\\
\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})
  = \rho \, \text{Cov}(\texttt{mo}_{t+h-1},\texttt{mo}_{t}) \\
\]</span>
Which we can use to recursively get at the covariance we need:</p>
<p><span class="math display">\[
\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})
  = \rho \, \text{Cov}(\texttt{mo}_{t+h-1},\texttt{mo}_{t})
\\
\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})
  = \rho \,( \rho \, \text{Cov}(\texttt{mo}_{t+h-2},\texttt{mo}_{t}) )
\\
\dots
\\
\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})
  = \rho^h \, \text{Cov}(\texttt{mo}_{t},\texttt{mo}_{t})
\\
\text{Cov}(\texttt{mo}_{t+h},\texttt{mo}_{t})
  = \rho^h \, \sigma_\texttt{mo}^2/(1 - \rho^2) \\
\]</span></p>
<p>Writing this in Stan code to replace lines 59 to 63 in
<code>stan/hier_NB_regression_ncp_slopes_mod_mos.stan</code> we would get:</p>
<pre><code>vector[M] mo;
{
  matrix[M,M] Sigma;
  for (m in 1:M) {
    Sigma[m,m] = 1.0;
    for (i in (m + 1):M) {
      Sigma[i,m] = rho^(i - m);
      Sigma[m,i] = Sigma[i,m];
    }
  }
  Sigma = Sigma * sigma_mo^2 / (1 - rho^2);
  mo = cholesky_decompose(Sigma) * mo_raw;
}</code></pre>
</div>
<div id="extension-to-gaussian-processes" class="section level3 unnumbered">
<h3>Extension to Gaussian processes</h3>
<p>The prior we defined for <code>mo</code> is strictly speaking a Gaussian
process. It is a stochastic process that is distributed as jointly
multivariate normal for any finite value of <span class="math inline">\(M\)</span>. Formally, we could
write the above prior for <code>mo</code> like so:</p>
<p><span class="math display">\[ \begin{aligned}
  \sigma_\texttt{mo} &amp; \sim \text{normal}(0, 1) \\
  \rho &amp; \sim \text{GenBeta}(-1,1,10, 5) \\
  \texttt{mo}_t &amp; \sim \text{GP}\left( 0,
  K(t | \sigma_\texttt{mo},\rho) \right) \\
\end{aligned} \]</span></p>
<p>The notation <span class="math inline">\(K(t | \sigma_\texttt{mo},\rho)\)</span> defines the covariance
matrix of the process over the domain <span class="math inline">\(t\)</span>, which is months.</p>
<p>In other words:</p>
<p><span class="math display">\[
\text{Cov}(\texttt{mo}_t,\texttt{mo}_{t+h})
  = k(t, t+h | \sigma_\texttt{mo}, \rho)
\]</span></p>
<p>We’ve already derived the covariance for our process. What if we want
to use a different definition of <code>Sigma</code>?</p>
<p>As the above example shows defining a proper covariance matrix will
yield a proper multivariate normal prior on a parameter. We need a way
of defining a proper covariance matrix. These are symmetric positive
definite matrices. It turns out there is a class of functions that
define proper covariance matrices, called <strong>kernel functions</strong>. These
functions are applied elementwise to construct a covariance matrix,
<span class="math inline">\(K\)</span>:
<span class="math display">\[
K_{[t,t+h]} = k(t, t+h | \theta)
\]</span>
where <span class="math inline">\(\theta\)</span> are the hyperparameters that define the behavior of
covariance matrix.</p>
<p>One such function is called the <strong>exponentiated quadratic function</strong>,
and it happens to be implemented in Stan as <code>cov_exp_quad</code>. The
function is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
k(t, t+h | \theta)
&amp; = \alpha^2  \exp \left( - \dfrac{1}{2\ell^2} ((t+h) - t)^2 \right)
\\
&amp; = \alpha^2  \exp \left( - \dfrac{h^2}{2\ell^2} \right)
\end{aligned}
\]</span></p>
<p>The exponentiated quadratic kernel has two components to theta,
<span class="math inline">\(\alpha\)</span>, the marginal standard deviation of the stochastic process
<span class="math inline">\(f\)</span> and <span class="math inline">\(\ell\)</span>, the process length sscale.</p>
<p>The length scale defines how quickly the covariance decays between
time points, with large values of <span class="math inline">\(\ell\)</span> yielding a covariance that
decays slowly, and with small values of <span class="math inline">\(\ell\)</span> yielding a covariance
that decays rapidly. It can be seen interpreted as a measure of how
nonlinear the <code>mo</code> process is in time.</p>
<p>The marginal standard deviation defines how large the fluctuations are
on the output side, which in our case is the number of roach
complaints per month across all buildings. It can be seen as a scale
parameter akin to the scale parameter for our building-level
hierarchical intercept, though it now defines the scale of the monthly
deviations.</p>
<p>This kernel’s defining quality is its smoothness; the function is
infinitely differentiable. That will present problems for our example,
but if we add some noise the diagonal of our covariance matrix, the
model will fit well.</p>
<p><span class="math display">\[
k(t, t+h | \theta)
 = \alpha^2  \exp \left( - \dfrac{h^2}{2\ell^2} \right)
   + \text{if } h = 0, \, \sigma^2_\texttt{noise} \text{ else } 0
\]</span></p>
</div>
<div id="compiling-the-gaussian-process-model" class="section level3 unnumbered">
<h3>Compiling the Gaussian process model</h3>
</div>
<div id="fitting-the-gaussian-process-model-to-data" class="section level3 unnumbered">
<h3>Fitting the Gaussian process model to data</h3>
</div>
<div id="examining-the-fit" class="section level3 unnumbered">
<h3>Examining the fit</h3>
<p>Let’s look at the prior vs. posterior for the Gaussian process length
scale parameter:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-52-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>From the plot above it only looks like we learned a small amount,
however we can see a bigger difference between the prior and posterior
if we consider how much we learned about the ratio of <code>sigma_gp</code> to
the length scale <code>gp_len</code>:</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-53-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This is a classic problem with Gaussian processes. Marginally, the
length scale parameter isn’t well identified by the data, but jointly
the length scale and the marginal standard deviation are well
identified.</p>
<p>And let’s compare the estimates for the time varying parameters
between the AR(1) and GP. In this case the posterior mean of the time
trend is essentially the same for the AR(1) and GP priors but the 50%
uncertainty intervals are narrower for the AR(1):</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-54-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The way we coded the Gaussian process also lets us plot a
decomposition of the GP into a monthly noise component (<code>mo_noise</code> in
the Stan code) and the underlying smoothly varying trend
(<code>gp_exp_quad</code> in the Stan code):</p>
<p><img src="workflow_files/figure-html/unnamed-chunk-55-1.png" width="70%" style="display: block; margin: auto;" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-examples.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="example-models-part.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
