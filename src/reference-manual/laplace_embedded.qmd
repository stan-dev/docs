---
pagetitle: Embedded Laplace Approximation
---

# Embdeed Laplace Approximation

Stan provides functions to perform an embedded Laplace 
approximation for latent Gaussian models. Bearing a slight abuse of language,
this is sometimes known as an integrated or nested Laplace approximation.
Details of Stan's implementation can be found in reference
[@Margossian:2020, @Margossian:2023].

A standard approach to fit a latent Gaussian model would be to perform inference
jointly over the latent Gaussian variables and the hyperparameters.
Instead, the embedded Laplace approximation can be used to do *approximate*
marginalization of the latent Gaussian variables; we can then
use any inference over the remaining hyperparameters, for example Hamiltonian
Monte Carlo sampling.

Formally, consider a latent Gaussian model,
$$
\begin{eqnarray*}
  \phi & \sim & p(\phi) \\
  \theta & \sim & \text{Multi-Normal}(0, K(\phi)) \\
  y & \sim & p(y \mid \theta, \phi).
\end{eqnarray*}
$$
The motivation for marginalization is to bypass the challenging geometry of the joint
posterior $p(\phi, \theta \mid y)$. This geometry (e.g. funnels) often frustrates
inference algorithms, including Hamiltonian Monte Carlo sampling and approximate
methods such as variational inference. On the other hand, the marginal posterior
$p(\phi \mid y)$ is often well-behaved and in many cases low-dimensional.
Furthermore, the conditional posterior $p(\theta \mid \phi, y)$ can be well 
approximated by a normal distribution, if the likelihood $p(y \mid \theta, \phi)$ 
is log concave.


## Approximation of the conditional posterior and marginal likelihood

The Laplace approximation is the normal distribution that matches the mode
and curvatureof the conditional posterior $p(\theta \mid y, \phi)$.
The mode,
$$
  \theta^* = \underset{\theta}{\text{argmax}} \ p(\theta \mid y, \phi),
$$
is estimated by a Newton solver. Since the approximation is normal, 
the curvature is matched by setting the covariance to the negative Hessian
of the log conditional posterior, evaluated at the mode,
$$
  \Sigma^* = -  \left . \frac{\partial^2}{\partial \theta^2} 
    \log p (\theta \mid \phi, y) \right |_{\theta =\theta^*}.
$$
The resulting Laplace approximation is then,
$$
\hat p_\mathcal{L} (\theta \mid y, \phi) = \text{Multi-Normal}(\theta^*, \Sigma^*)
\approx p(\theta \mid y, \phi).
$$
This approximation implies another approximation for the marginal likelihood,
$$
   \hat p_\mathcal{L}(y \mid \phi) := \frac{p(\theta^* \mid \phi) \ 
   p(y \mid \theta^*, \phi) }{ \hat p_\mathcal{L} (\theta^* \mid \phi, y) }
   \approx p(y \mid \phi).
$$
Hence, a strategy to approximate the posterior of the latent Gaussian model
is to first estimate the marginal posterior 
$\hat p_\mathcal{L}(\phi \mid y) \propto p(\phi) p_\mathcal{L} (y \mid \phi)$ 
using any algorithm supported by Stan.
Approximate posterior draws for the latent Gaussian variables are then
obtained by first drawing $\phi \sim \hat p_\mathcal{L}(\phi \mid y)$ and
then $\theta \sim  hat p_\mathcal{L}(\theta \mid \phi, y)$.


## Trade-offs of the approximation

The embedded Laplace approximation presents several trade-offs with standard
inference over the joint posterior $p(\theta, \phi \mid y)$. The main
advantage of the embedded Laplace approximation is that it side-steps the
intricate geometry of hierarchical models. The marginal posterior 
$p(\phi \mid y)$ can then be handled by Hamiltonian Monte Carlo sampling
without extensive tuning or reparameterization, and the mixing time is faster,
meaning we can run shorter chains to achieve a desired precision. In some cases,
approximate methods, e.g. variational inference, which
work poorly on the joint $p(\theta, \phi \mid y)$ work well on the marginal
posterior $p(\phi \mid y)$.

On the other hand, the embedded Laplace approximation presents certain
disadvantages. First, we need to perform a Laplace approximation each time
the log marginal likelihood is evaluated, meaning each iteration
can be expensive. Secondly, the approximation can introduce non-negligable
error, especially with non-conventional likelihoods (note the prior
is always multivariate normal). How these trade-offs are resolved depends on the application; see [@Margossian:2020] for some examples.


## Details of the approximation

### Tuning the Newton solver

A critical component of the embedded Laplace approximation is the Newton solver
used to estimate the mode $\theta^*$ of $p(\theta \mid \phi, y)$. The objective
function being maximized is
$$
\Psi(\theta) = \log p(\theta \mid \phi) + \log p(y \mid \theta, \phi),
$$
and convergence is declared if the change in the objective is sufficiently
small between two iterations
$$
| \Psi (\theta^{(i + 1)}) - \Psi (\theta^{(i)}) | \le \Delta, 
$$
for some *tolerance* $\Delta$. The solver also stops after reaching a
pre-specified *maximum number of steps*: in that case, Stan throws an exception
and rejects the current proposal. This is not a problem, as
long as these exceptions are rare and confined to early phases of the warmup.

The Newton iteration can be augmented with a linesearch step to insure that
at each iteration the objective function $\Psi$ decreases. Specifically,
suppose that
$$
\Psi (\theta^{(i + 1)}) < \Psi (\theta^{(i)}).
$$
This can indicate that the Newton step is too large and that we skipped a region
where the objective function decreases. In that case, we can reduce the step
length by a factor of 2, using
$$
  \theta^{(i + 1)} \leftarrow \frac{\theta^{(i + 1)} + \theta^{(i)}}{2}. 
$$
We repeat this halving of steps until 
$\Psi (\theta^{(i + 1)}) \ge \Psi (\theta^{(i)})$, or until a maximum number
of linesearch steps is reached. By defaut, this maximum is set to 0, which
means the Newton solver performs no linesearch. For certain problems, adding
a linsearch can make the optmization more stable.


The embedded Laplace approximation uses a custom Newton solver,specialized 
to find the mode of $p(\theta \mid \phi, y)$.
A keystep for efficient optimization is to insure all matrix inversions are
numerically stable. This can be done using the Woodburry-Sherman-Morrison
formula and requires one of three matrix decompositions:

1. Cholesky decomposition of the Hessian of the negative log likelihood
$W = - \partial^2_\theta \log p(y \mid \theta, \phi)$

2. Cholesky decomposition of the prior covariance matrix $K(\phi)$.

3. LU-decomposition of $I + KW$, where $I$ is the identity matrix.

The first solver (1) should be used if the negative log likelihood is
positive-definite. Otherwise the user should rely on (2). In rarer cases where
it is not numerically safe to invert the covariance matrix $K$, users can
use the third solver as a last-resort option.


### Sparse Hessian of the log likelihood

A key step to speed up computation is to take advantage of the sparsity of
the Hessian of the log likelihood,
$$
  H = \frac{\partial^2}{\partial \theta^2} \log p(y \mid \theta, \phi).
$$
For example, if the observations $(y_1, \cdots, y_N)$ are conditionally
independent and each depends on only depend on one component of $\theta$,
such that
$$
  \log p(y \mid \theta, \phi) = \sum_{i = 1}^N \log p(y_i \mid \theta_i, \phi),
$$
then the Hessian is diagonal. This leads to faster calculations of the Hessian
and subsequently sparse matrix operations. This case is common in Gaussian
process models, and certain hierarchical models.

Stan's suite of functions for the embedded Laplace approximation are not
equipped to handle arbitrary sparsity structures; instead, they work on
block-diagonal Hessians, and the user can specify the size $B$ of these blocks.
The user is responsible for working out what $B$ is. If the Hessian is dense,
then we simply set $B = N$.

NOTE: currently, there is no support for sparse prior covariance matrix.
We expect this to be supported in future versions of Stan.
